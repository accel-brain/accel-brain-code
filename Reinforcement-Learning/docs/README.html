<h1 id="reinforcement-learning-library-pyqlearning">Reinforcement
Learning Library: pyqlearning</h1>
<p><code>pyqlearning</code> is Python library to implement Reinforcement
Learning and Deep Reinforcement Learning, especially for Q-Learning,
Deep Q-Network, and Multi-agent Deep Q-Network which can be optimized by
Annealing models such as Simulated Annealing, Adaptive Simulated
Annealing, and Quantum Monte Carlo Method.</p>
<p>This library makes it possible to design the information search
algorithm such as the Game AI, web crawlers, or Robotics. But this
library provides components for designers, not for end-users of
state-of-the-art black boxes. Briefly speaking the philosophy of this
library, <em>give user hype-driven blackboxes and you feed him for a
day; show him how to design algorithms and you feed him for a
lifetime.</em> So algorithm is power.</p>
<div data-align="center">
<p>
<a href="https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/search_maze_by_deep_q_network.ipynb" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/DQN_single_agent_goal_compressed-loop.gif" /></a>
</p>
<p>
Deep Reinforcement Learning (Deep Q-Network: DQN) to solve Maze.
</p>
</div>
<div data-align="center">
<p>
<a href="https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/search_maze_by_deep_q_network.ipynb" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/DQN_multi_agent_demo_goal_enemy_2-compressed.gif" /></a>
</p>
<p>
Multi-agent Deep Reinforcement Learning to solve the pursuit-evasion
game.
</p>
</div>
<h2 id="installation">Installation</h2>
<p>Install using pip:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pyqlearning</span></code></pre></div>
<h3 id="source-code">Source code</h3>
<p>The source code is currently hosted on GitHub.</p>
<ul>
<li><a
href="https://github.com/chimera0/accel-brain-code/tree/master/Reinforcement-Learning">accel-brain-code/Reinforcement-Learning</a></li>
</ul>
<h3 id="python-package-indexpypi">Python package index(PyPI)</h3>
<p>Installers for the latest released version are available at the
Python package index.</p>
<ul>
<li><a href="https://pypi.python.org/pypi/pyqlearning/">pyqlearning :
Python Package Index</a></li>
</ul>
<h3 id="dependencies">Dependencies</h3>
<ul>
<li><a href="https://github.com/numpy/numpy">numpy</a>: v1.13.3 or
higher.</li>
<li><a href="https://github.com/pandas-dev/pandas">pandas</a>: v0.22.0
or higher.</li>
</ul>
<h4 id="option">Option</h4>
<ul>
<li><a
href="https://github.com/accel-brain/accel-brain-code/tree/master/Accel-Brain-Base">accel-brain-base</a>:
v1.0.0 or higher.
<ul>
<li>Only if you want to implement the <em>Deep</em> Reinforcement
Learning.</li>
</ul></li>
<li><a href="https://github.com/apache/incubator-mxnet">mxnet</a> or <a
href="https://mxnet.apache.org/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html">mxnet-cu*</a>:
latest.
<ul>
<li>Only when building a model of this library using <a
href="https://mxnet.apache.org/">Apache MXNet</a>.</li>
</ul></li>
<li><a href="https://pytorch.org/get-started/locally/">torch</a>
<ul>
<li>Only when building a model of this library using <a
href="https://pytorch.org/">PyTorch</a>.</li>
</ul></li>
</ul>
<h2 id="documentation">Documentation</h2>
<p>Full documentation is available on <a
href="https://code.accel-brain.com/Reinforcement-Learning/">https://code.accel-brain.com/Reinforcement-Learning/</a>
. This document contains information on functionally reusability,
functional scalability and functional extensibility.</p>
<h2 id="description">Description</h2>
<p><code>pyqlearning</code> is Python library to implement Reinforcement
Learning and Deep Reinforcement Learning, especially for Q-Learning,
Deep Q-Network, and Multi-agent Deep Q-Network which can be optimized by
Annealing models such as Simulated Annealing, Adaptive Simulated
Annealing, and Quantum Monte Carlo Method.</p>
<p>This library provides components for designers, not for end-users of
state-of-the-art black boxes. Reinforcement learning algorithms are
highly variable because they must design single or multi-agent behavior
depending on their problem setup. Designers of algorithms and
architectures are required to design according to the situation at each
occasion. Commonization and commoditization for end users who want
easy-to-use tools is not easy. Nonetheless, commonality / variability
analysis and object-oriented analysis are not impossible. I am convinced
that a designer who can <em>practice</em> <em>abstraction</em> of
concepts by <em>drawing a distinction</em> of concepts related to
his/her <em>own concrete problem settings</em> makes it possible to
distinguish commonality and variability of various Reinforcement
Learning algorithms.</p>
<h3
id="the-commonalityvariability-of-epsilon-greedy-q-leanring-and-boltzmann-q-learning">The
commonality/variability of Epsilon Greedy Q-Leanring and Boltzmann
Q-Learning</h3>
<p>According to the Reinforcement Learning problem settings, Q-Learning
is a kind of <strong>Temporal Difference learning(TD Learning)</strong>
that can be considered as hybrid of <strong>Monte Carlo</strong> method
and <strong>Dynamic Programming</strong> method. As Monte Carlo method,
TD Learning algorithm can learn by experience without model of
environment. And this learning algorithm is functional extension of
bootstrap method as Dynamic Programming Method.</p>
<p>In this library, Q-Learning can be distinguished into <strong>Epsilon
Greedy Q-Leanring</strong> and <strong>Boltzmann Q-Learning</strong>.
These algorithm is functionally equivalent but their structures should
be conceptually distinguished.</p>
<p>Epsilon Greedy Q-Leanring algorithm is a typical off-policy
algorithm. In this paradigm, <em>stochastic</em> searching and
<em>deterministic</em> searching can coexist by hyperparameter
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/epsilon.gif" />
that is probability that agent searches greedy. Greedy searching is
<em>deterministic</em> in the sense that policy of agent follows the
selection that maximizes the Q-Value.</p>
<p>Boltzmann Q-Learning algorithm is based on Boltzmann action selection
mechanism, where the probability
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/x_i.gif" />
of selecting the action
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i.gif" />
is given by</p>
<!-- $$x_i(t) = \frac{e^{\frac{Q_i(t)}{T}}}{\sum_{k}^{ } e^{\frac{Q_i(t)}{T}}} \ \  (i = 1, 2, ..., n)$$ -->
<div>
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/boltzmann_action_selection.gif" />
</div>
<p>where the temperature
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/t_gt_0.gif" />
controls exploration/exploitation tradeoff. For
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/t_to_0.gif" />
the agent always acts greedily and chooses the strategy corresponding to
the maximum Q–value, so as to be pure <em>deterministic</em>
exploitation, whereas for
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/t_to_infty.gif" />
the agent’s strategy is completely random, so as to be pure
<em>stochastic</em> exploration.</p>
<h3
id="commonalityvariability-of-q-learning-models">Commonality/variability
of Q-learning models</h3>
<p>Considering many variable parts and functional extensions in the
Q-learning paradigm from perspective of <em>commonality/variability
analysis</em> in order to practice object-oriented design, this library
provides abstract class that defines the skeleton of a Q-Learning
algorithm in an operation, deferring some steps in concrete variant
algorithms such as Epsilon Greedy Q-Leanring and Boltzmann Q-Learning to
client subclasses. The abstract class in this library lets subclasses
redefine certain steps of a Q-Learning algorithm without changing the
algorithm’s structure.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/class_diagram_pyqleanring_QLearning.png" /></p>
<p>Typical concepts such as <code>State</code>, <code>Action</code>,
<code>Reward</code>, and <code>Q-Value</code> in Q-learning models
should be refered as viewpoints for distinguishing between
<em>commonality</em> and <em>variability</em>. Among the functions
related to these concepts, the class <code>QLearning</code> is
responsible for more <em>common</em> attributes and behaviors. On the
other hand, in relation to <em>your</em> concrete problem settings, more
<em>variable</em> elements have to be implemented by subclasses such as
<code>YourGreedyQLearning</code> or
<code>YourBoltzmannQLearning</code>.</p>
<p>For more detailed specification of this template method, refer to API
documentation: <a
href="https://code.accel-brain.com/Reinforcement-Learning/pyqlearning.html#module-pyqlearning.q_learning">pyqlearning.q_learning
module</a>. If you want to know the samples of implemented code, see <a
href="https://github.com/chimera0/accel-brain-code/tree/master/Reinforcement-Learning/demo">demo/</a>.</p>
<h3 id="structural-extension-deep-reinforcement-learning">Structural
extension: Deep Reinforcement Learning</h3>
<p>The Reinforcement learning theory presents several issues from a
perspective of deep learning theory(Mnih, V., et al. 2013). Firstly,
deep learning applications have required large amounts of hand-labelled
training data. Reinforcement learning algorithms, on the other hand,
must be able to learn from a scalar reward signal that is frequently
sparse, noisy and delayed.</p>
<p>The difference between the two theories is not only the type of data
but also the timing to be observed. The delay between taking actions and
receiving rewards, which can be thousands of timesteps long, seems
particularly daunting when compared to the direct association between
inputs and targets found in supervised learning.</p>
<p>Another issue is that deep learning algorithms assume the data
samples to be independent, while in reinforcement learning one typically
encounters sequences of highly correlated states. Furthermore, in
Reinforcement learning, the data distribution changes as the algorithm
learns new behaviours, presenting aspects of <em>recursive
learning</em>, which can be problematic for deep learning methods that
assume a fixed underlying distribution.</p>
<h4 id="generalisation-or-a-function-approximation">Generalisation, or a
function approximation</h4>
<p>This library considers problem setteing in which an agent interacts
with an environment
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/mathcal_E.png" />,
in a sequence of actions, observations and rewards. At each time-step
the agent selects an action at from the set of possible actions,
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/A_1_K.png" />.
The state/action-value function is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_s_a.png" />.</p>
<p>The goal of the agent is to interact with the
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/mathcal_E.png" />
by selecting actions in a way that maximises future rewards. We can make
the standard assumption that future rewards are discounted by a factor
of <span class="math inline"><em>γ</em></span> per time-step, and define
the future discounted return at time
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/t.png" />
as</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/r_t_sum_t_t_T_gamma.png" />,</p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Tt.png" />
is the time-step at which the agent will reach the goal. This library
defines the optimal state/action-value function
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_ast_s_a.png" />
as the maximum expected return achievable by following any strategy,
after seeing some state
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/s.png" />
and then taking some action
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/a.png" />,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_ast_s_a_max_pi_E.png" />,</p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/pi.png" />
is a policy mapping sequences to actions (or distributions over
actions).</p>
<p>The optimal state/action-value function obeys an important identity
known as the Bellman equation. This is based on the following
<em>intuition</em>: if the optimal value
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_ast_s_d_a_d.png" />
of the sequence
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/s_d.png" />
at the next time-step was known for all possible actions
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/a_d.png" />,
then the optimal strategy is to select the action
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/a_d.png" />
maximising the expected value of</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/r_gamma_Q_ast_s_d_a_d.png" />,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_ast_s_d_a_d_mathbb_E_s_d_sim_mathcal_E.png" />.</p>
<p>The basic idea behind many reinforcement learning algorithms is to
estimate the state/action-value function, by using the Bellman equation
as an iterative update,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_i_1_s_a_mathbb_E_r_gamma_max_a_d.png" />.</p>
<p>Such <em>value iteration algorithms</em> converge to the optimal
state/action-value function,
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_i_rightarrow_Q_ast.png" />
as
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i_rightarrow_infty.png" />.</p>
<p>But increasing the complexity of states/actions is equivalent to
increasing the number of combinations of states/actions. If the value
function is continuous and granularities of states/actions are extremely
fine, the combinatorial explosion will be encountered. In other words,
this basic approach is totally impractical, because the
state/action-value function is estimated separately for each sequence,
without any <strong>generalisation</strong>. Instead, it is common to
use a <strong>function approximator</strong> to estimate the
state/action-value function,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/Q_s_a_theta_approx_Q_ast_s_a.png" /></p>
<p>So the Reduction of complexities is required.</p>
<h3 id="deep-q-network">Deep Q-Network</h3>
<p>In this problem setting, the function of nerual network or deep
learning is a function approximation with weights
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/theta.png" />
as a Q-Network. A Q-Network can be trained by minimising a loss
functions
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/L_i_theta_i.png" />
that changes at each iteration
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i.png" />,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/L_i_theta_i_mathbb_E_s_a_sim_rho_cdot.png" /></p>
<p>where</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/y_i_mathbb_E_s_d_sim_mathcal_E_r_gamma_max_a_d.png" /></p>
<p>is the target for iteration
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i.png" />
and
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/rho_cdot.png" />
is a so-called behaviour distribution. This is probability distribution
over states and actions. The parameters from the previous iteration
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/theta_i_1.png" />
are held fixed when optimising the loss function
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/L_i_theta_i.png" />.
Differentiating the loss function with respect to the weights we arrive
at the following gradient,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/nabla_theta_i_L_i_theta_i_mathbb_E_s_a_sim_rho_cdot.png" /></p>
<h2
id="tutorial-maze-solving-and-the-pursuit-evasion-game-by-deep-q-network-jupyter-notebook">Tutorial:
Maze Solving and the pursuit-evasion game by Deep Q-Network (Jupyter
notebook)</h2>
<p><a
href="https://github.com/accel-brain/accel-brain-code/blob/master/Reinforcement-Learning/demo/search_maze_by_deep_q_network.ipynb">demo/search_maze_by_deep_q_network.ipynb</a>
is a Jupyter notebook which demonstrates a maze solving algorithm based
on Deep Q-Network, rigidly coupled with Deep Convolutional Neural
Networks(Deep CNNs). The function of the Deep Learning is
<strong>generalisation</strong> and CNNs is-a <strong>function
approximator</strong>. In this notebook, several functional equivalents
such as CNN and LSTM can be compared from a functional point of
view.</p>
<div data-align="center">
<pre><code>&lt;p&gt;&lt;a href=&quot;https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/search_maze_by_deep_q_network.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/DQN_single_agent_goal_compressed-loop.gif&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deep Reinforcement Learning to solve the Maze.&lt;/p&gt;</code></pre>
</div>
<ul>
<li>Black squares represent a wall.</li>
<li>Light gray squares represent passages.</li>
<li>A dark gray square represents a start point.</li>
<li>A white squeare represents a goal point.</li>
</ul>
<h3 id="the-pursuit-evasion-game">The pursuit-evasion game</h3>
<p>Expanding the search problem of the maze makes it possible to
describe the pursuit-evasion game that is a family of problems in
mathematics and computer science in which one group attempts to track
down members of another group in an environment.</p>
<p>This problem can be re-described as the multi-agent control problem,
which involves decomposing the global system state into an image like
representation with information encoded in separate channels. This
reformulation allows us to use convolutional neural networks to
efficiently extract important features from the image-like state.</p>
<p>Egorov, M. (2016) and Gupta, J. K. et al.(2017) proposed new
algorithm which uses the image-like state representation of the
multi-agent system as an input, and outputs the estimated Q-values for
the agent in question. They described a number of implementation
contributions that make training efficient and allow agents to learn
directly from the behavior of other agents in the system.</p>
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/multi_agent_q_learning_and_channels_big.png" />
<p>
<cite><a href="https://pdfs.semanticscholar.org/dd98/9d94613f439c05725bad958929357e365084.pdf" target="_blank">Egorov,
M. (2016). Multi-agent deep reinforcement learning., p4.</a></cite>
</p>
<p>An important aspect of this data modeling is that by expressing each
state of the multi-agent as channels, it is possible to enclose states
of all the agents as <strong>a target of convolution operation all at
once</strong>. By the affine transformation executed by the neural
network, combinations of an enormous number of states of multi-agent can
be computed in principle with an allowable range of memory.</p>
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/multi_agent_q_learning_and_cnn_model_big.png" />
<p>
<cite><a href="https://pdfs.semanticscholar.org/dd98/9d94613f439c05725bad958929357e365084.pdf" target="_blank">Egorov,
M. (2016). Multi-agent deep reinforcement learning., p4.</a></cite>
</p>
<p><a
href="https://github.com/accel-brain/accel-brain-code/blob/master/Reinforcement-Learning/demo/multi_agent_maze_by_deep_q_network.ipynb">demo/multi_agent_maze_by_deep_q_network.ipynb</a>
also prototypes Multi Agent Deep Q-Network to solve the pursuit-evasion
game based on the image-like state representation of the
multi-agent.</p>
<div data-align="center">
<pre><code>&lt;table style=&quot;border: none;&quot;&gt;
    &lt;tr&gt;
        &lt;td width=&quot;45%&quot; align=&quot;center&quot;&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/accel-brain/accel-brain-code/blob/master/Reinforcement-Learning/demo/multi_agent_maze_by_deep_q_network.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/DQN_multi_agent_demo_crash_enemy_2-compressed.gif&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
        &lt;p&gt;Multi-agent Deep Reinforcement Learning to solve the pursuit-evasion game. The player is caught by enemies.&lt;/p&gt;
        &lt;/td&gt;
        &lt;td width=&quot;45%&quot; align=&quot;center&quot;&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/accel-brain/accel-brain-code/blob/master/Reinforcement-Learning/demo/multi_agent_maze_by_deep_q_network.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/DQN_multi_agent_demo_goal_enemy_2-compressed.gif&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
        &lt;p&gt;
        &lt;p&gt;Multi-agent Deep Reinforcement Learning to solve the pursuit-evasion game. The player reaches the goal.&lt;/p&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;</code></pre>
</div>
<ul>
<li>Black squares represent a wall.</li>
<li>Light gray squares represent passages.</li>
<li>A dark gray square represents a start point.</li>
<li>Moving dark gray squares represent enemies.</li>
<li>A white squeare represents a goal point.</li>
</ul>
<h2
id="tutorial-complexity-of-hyperparameters-or-how-can-be-hyperparameters-decided">Tutorial:
Complexity of Hyperparameters, or how can be hyperparameters
decided?</h2>
<p>There are many hyperparameters that we have to set before the actual
searching and learning process begins. Each parameter should be decided
in relation to Deep/Reinforcement Learning theory and it cause side
effects in training model. Because of this complexity of
hyperparameters, so-called the hyperparameter tuning must become a
burden of Data scientists and R &amp; D engineers from the perspective
of not only a theoretical point of view but also implementation
level.</p>
<h3
id="combinatorial-optimization-problem-and-simulated-annealing.">Combinatorial
optimization problem and Simulated Annealing.</h3>
<p>This issue can be considered as <strong>Combinatorial optimization
problem</strong> which is an optimization problem, where an optimal
solution has to be identified from a finite set of solutions. The
solutions are normally discrete or can be converted into discrete. This
is an important topic studied in operations research such as software
engineering, artificial intelligence(AI), and machine learning. For
instance, travelling sales man problem is one of the popular
combinatorial optimization problem.</p>
<p>In this problem setting, this library provides an Annealing Model to
search optimal combination of hyperparameters. For instance,
<strong>Simulated Annealing</strong> is a probabilistic single solution
based search method inspired by the annealing process in metallurgy.
Annealing is a physical process referred to as tempering certain alloys
of metal, glass, or crystal by heating above its melting point, holding
its temperature, and then cooling it very slowly until it solidifies
into a perfect crystalline structure. The simulation of this process is
known as simulated annealing.</p>
<h3 id="functional-comparison.">Functional comparison.</h3>
<p><a
href="https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/annealing_hand_written_digits.ipynb">demo/annealing_hand_written_digits.ipynb</a>
is a Jupyter notebook which demonstrates a very simple classification
problem: Recognizing hand-written digits, in which the aim is to assign
each input vector to one of a finite number of discrete categories, to
learn observed data points from already labeled data how to predict the
class of unlabeled data. In the usecase of hand-written digits dataset,
the task is to predict, given an image, which digit it represents.</p>
<p>There are many structural extensions and functional equivalents of
<strong>Simulated Annealing</strong>. For instance, <strong>Adaptive
Simulated Annealing</strong>, also known as the very fast simulated
reannealing, is a very efficient version of simulated annealing. And
<strong>Quantum Monte Carlo</strong>, which is generally known a
stochastic method to solve the Schrödinger equation, is one of the
earliest types of solution in order to simulate the <strong>Quantum
Annealing</strong> in classical computer. In summary, one of the
function of this algorithm is to solve the ground state search problem
which is known as logically equivalent to combinatorial optimization
problem. Then this Jupyter notebook demonstrates functional comparison
in the same problem setting.</p>
<h2
id="demonstration-epsilon-greedy-q-learning-and-simulated-annealing.">Demonstration:
Epsilon Greedy Q-Learning and Simulated Annealing.</h2>
<p>Import python modules.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyqlearning.annealingmodel.costfunctionable.greedy_q_learning_cost <span class="im">import</span> GreedyQLearningCost</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyqlearning.annealingmodel.simulated_annealing <span class="im">import</span> SimulatedAnnealing</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># See demo/demo_maze_greedy_q_learning.py</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> demo.demo_maze_greedy_q_learning <span class="im">import</span> MazeGreedyQLearning</span></code></pre></div>
<p>The class <code>GreedyQLearningCost</code> is implemented the
interface <code>CostFunctionable</code> to be called by
<code>AnnealingModel</code>. This cost function is defined by</p>
<div>
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/q_cost.gif">
</div>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/n_search.gif">
is the number of searching(learning) and L is a limit of
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/n_search.gif">.</p>
<p>Like Monte Carlo method, let us draw random samples from a normal
(Gaussian) or unifrom distribution.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Epsilon-Greedy rate in Epsilon-Greedy-Q-Learning.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>greedy_rate_arr <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.5</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Alpha value in Q-Learning.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>alpha_value_arr <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.5</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Gamma value in Q-Learning.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>gamma_value_arr <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.5</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Limit of the number of Learning(searching).</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>limit_arr <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">10</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>var_arr <span class="op">=</span> np.c_[greedy_rate_arr, alpha_value_arr, gamma_value_arr, limit_arr]</span></code></pre></div>
<p>Instantiate and initialize <code>MazeGreedyQLearning</code> which
is-a <code>GreedyQLearning</code>.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiation.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>greedy_q_learning <span class="op">=</span> MazeGreedyQLearning()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>greedy_q_learning.initialize(hoge<span class="op">=</span>fuga)</span></code></pre></div>
<p>Instantiate <code>GreedyQLearningCost</code> which is implemented the
interface <code>CostFunctionable</code> to be called by
<code>AnnealingModel</code>.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>init_state_key <span class="op">=</span> (<span class="st">&quot;Some&quot;</span>, <span class="st">&quot;data&quot;</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cost_functionable <span class="op">=</span> GreedyQLearningCost(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    greedy_q_learning, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    init_state_key<span class="op">=</span>init_state_key</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Instantiate <code>SimulatedAnnealing</code> which is-a
<code>AnnealingModel</code>.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>annealing_model <span class="op">=</span> SimulatedAnnealing(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># is-a `CostFunctionable`.</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    cost_functionable<span class="op">=</span>cost_functionable,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The number of annealing cycles.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    cycles_num<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The number of trials of searching per a cycle.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    trials_per_cycle<span class="op">=</span><span class="dv">3</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Fit the <code>var_arr</code> to <code>annealing_model</code>.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>annealing_model.var_arr <span class="op">=</span> var_arr</span></code></pre></div>
<p>Start annealing.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>annealing_model.annealing()</span></code></pre></div>
<p>To extract result of searching, call the property
<code>predicted_log_list</code> which is list of tuple:
<code>(Cost, Delta energy, Mean of delta energy, probability in Boltzmann distribution, accept flag)</code>.
And refer the property <code>x</code> which is <code>np.ndarray</code>
that has combination of hyperparameters. The optimal combination can be
extracted as follow.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract list: [(Cost, Delta energy, Mean of delta energy, probability, accept)]</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>predicted_log_arr <span class="op">=</span> annealing_model.predicted_log_arr</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># [greedy rate, Alpha value, Gamma value, Limit of the number of searching.]</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>min_e_v_arr <span class="op">=</span> annealing_model.var_arr[np.argmin(predicted_log_arr[:, <span class="dv">2</span>])]</span></code></pre></div>
<h3 id="contingency-of-definitions">Contingency of definitions</h3>
<p>The above definition of cost function is possible option: not
necessity but contingent from the point of view of modal logic. You
should questions the necessity of definition and re-define, for
designing the implementation of interface <code>CostFunctionable</code>,
in relation to <em>your</em> problem settings.</p>
<h2
id="demonstration-epsilon-greedy-q-learning-and-adaptive-simulated-annealing.">Demonstration:
Epsilon Greedy Q-Learning and Adaptive Simulated Annealing.</h2>
<p>There are various Simulated Annealing such as Boltzmann Annealing,
Adaptive Simulated Annealing(SAS), and Quantum Simulated Annealing. On
the premise of Combinatorial optimization problem, these annealing
methods can be considered as functionally equivalent. The
<em>Commonality/Variability</em> in these methods are able to keep
responsibility of objects all straight as the class diagram below
indicates.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/class_diagram_annealing_model.png" /></p>
<h3 id="code-sample.">Code sample.</h3>
<p><code>AdaptiveSimulatedAnnealing</code> is-a subclass of
<code>SimulatedAnnealing</code>. The <em>variability</em> is aggregated
in the method <code>AdaptiveSimulatedAnnealing.adaptive_set()</code>
which must be called before executing
<code>AdaptiveSimulatedAnnealing.annealing()</code>.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyqlearning.annealingmodel.simulatedannealing.adaptive_simulated_annealing <span class="im">import</span> AdaptiveSimulatedAnnealing</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>annealing_model <span class="op">=</span> AdaptiveSimulatedAnnealing(</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    cost_functionable<span class="op">=</span>cost_functionable,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    cycles_num<span class="op">=</span><span class="dv">33</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    trials_per_cycle<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    accepted_sol_num<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    init_prob<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    final_prob<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    start_pos<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    move_range<span class="op">=</span><span class="dv">3</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Variability part.</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>annealing_model.adaptive_set(</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># How often will this model reanneals there per cycles.</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    reannealing_per<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Thermostat.</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    thermostat<span class="op">=</span><span class="fl">0.</span>,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The minimum temperature.</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    t_min<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The default temperature.</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    t_default<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>annealing_model.var_arr <span class="op">=</span> params_arr</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>annealing_model.annealing()</span></code></pre></div>
<p>To extract result of searching, call the property like the case of
using <code>SimulatedAnnealing</code>. If you want to know how to
visualize the searching process, see my Jupyter notebook: <a
href="https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/annealing_hand_written_digits.ipynb">demo/annealing_hand_written_digits.ipynb</a>.</p>
<h2
id="demonstration-epsilon-greedy-q-learning-and-quantum-monte-carlo.">Demonstration:
Epsilon Greedy Q-Learning and Quantum Monte Carlo.</h2>
<p>Generally, Quantum Monte Carlo is a stochastic method to solve the
Schrödinger equation. This algorithm is one of the earliest types of
solution in order to simulate the Quantum Annealing in classical
computer. In summary, one of the function of this algorithm is to solve
the ground state search problem which is known as logically equivalent
to combinatorial optimization problem.</p>
<p>According to theory of spin glasses, the ground state search problem
can be described as minimization energy determined by the hamiltonian
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/h_0.png" />
as follow</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/hamiltonian_in_ising_model.png" /></p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/pauli_z_i.png" />
refers to the Pauli spin matrix below for the spin-half particle at
lattice point
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i.gif" />.
In spin glasses, random value is assigned to
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/j_i_j.png" />.
The number of combinations is enormous. If this value is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/n.png" />,
a trial frequency is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/2_n.png" />.
This computation complexity makes it impossible to solve the ground
state search problem. Then, in theory of spin glasses, the standard
hamiltonian is re-described in expanded form.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/hamiltonian_in_t_ising_model.png" /></p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/pauli_x_i.png" />
also refers to the Pauli spin matrix and
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/gamma.png" />
is so-called annealing coefficient, which is hyperparameter that
contains vely high value. Ising model to follow this Hamiltonian is
known as the Transverse Ising model.</p>
<p>In relation to this system, thermal equilibrium amount of a physical
quantity
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/q.png?1" />
is as follow.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/langle_q_rangle.png" /></p>
<p>If
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/h.png" />
is a diagonal matrix, then also
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/e_beta_h.png" />
is diagonal matrix. If diagonal element in
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/h.png" />
is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/e_i.png" />,
Each diagonal element is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/e_beta_h_ij_e_i.png" />.
However if
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/h.png" />
has off-diagonal elements, It is known that
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/e_beta_h_ij_e_i_neq.png" />
since for any of the exponent
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/i.gif" />
we must exponentiate the matrix as follow.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/e_matrix_infty.png" /></p>
<p>Therefore, a path integration based on Trotter-Suzuki decomposition
has been introduced in Quantum Monte Carlo Method. This path integration
makes it possible to obtain the partition function
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z.png" />.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z_in_t_ising_model.png" /></p>
<p>where if
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/m.png" />
is large enough, relational expression below is established.</p>
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/exp_left_frac_1_m_beta_h_right.png" />
</td>
</tr>
<p>Then the partition function
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z.png" />
can be re-descibed as follow.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z_in_t_ising_model_re_described.png" /></p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/mid_sigma_k_rangle.png" />
is
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/l.png" />
topological products (product spaces). Because
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/h_0.png" />
is the diagonal matrix,
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/tilde_sigma_j_z_mid_sigma.png" />.</p>
<p>Therefore,</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/langle_sigma_k_mid.png" /></p>
<p>The partition function
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z.png" />
can be re-descibed as follow.</p>
<p><img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/z_in_t_ising_model_re_described_last.png" /></p>
<p>where
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/m.png" />
is the number of trotter.</p>
<p>This relational expression indicates that the quantum - mechanical
Hamiltonian in
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/d.png" />
dimentional Tranverse Ising model is functional equivalence to classical
Hamiltonian in
<img src="https://storage.googleapis.com/accel-brain-code/Reinforcement-Learning/img/latex/d_1.png" />
dimentional Ising model, which means that the state of the quantum -
mechanical system can be approximate by the state of classical
system.</p>
<h3 id="code-sample.-1">Code sample.</h3>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyqlearning.annealingmodel.quantum_monte_carlo <span class="im">import</span> QuantumMonteCarlo</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyqlearning.annealingmodel.distancecomputable.cost_as_distance <span class="im">import</span> CostAsDistance</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># User defined function which is-a `CostFuntionable`.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>cost_functionable <span class="op">=</span> YourCostFunctions()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cost as distance for `QuantumMonteCarlo`.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>distance_computable <span class="op">=</span> CostAsDistance(params_arr, cost_functionable)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Init.</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>annealing_model <span class="op">=</span> QuantumMonteCarlo(</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    distance_computable<span class="op">=</span>distance_computable,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The number of annealing cycles.</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    cycles_num<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inverse temperature (Beta).</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    inverse_temperature_beta<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gamma. (so-called annealing coefficient.) </span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    gammma<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attenuation rate for simulated time.</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    fractional_reduction<span class="op">=</span><span class="fl">0.99</span>,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The dimention of Trotter.</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    trotter_dimention<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The number of Monte Carlo steps.</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    mc_step<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The number of parameters which can be optimized.</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    point_num<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Default `np.ndarray` of 2-D spin glass in Ising model.</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    spin_arr<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tolerance for the optimization.</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># When the ΔE is not improving by at least `tolerance_diff_e`</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for two consecutive iterations, annealing will stops.</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    tolerance_diff_e<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Execute annealing.</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>annealing_model.annealing()</span></code></pre></div>
<p>To extract result of searching, call the property like the case of
using <code>SimulatedAnnealing</code>. If you want to know how to
visualize the searching process, see my Jupyter notebook: <a
href="https://github.com/chimera0/accel-brain-code/blob/master/Reinforcement-Learning/demo/annealing_hand_written_digits.ipynb">demo/annealing_hand_written_digits.ipynb</a>.</p>
<h2 id="references">References</h2>
<p>The basic concepts, theories, and methods behind this library are
described in the following books.</p>
<div data-align="center">
<a href="https://www.amazon.co.jp/dp/B08PV4ZQG5/" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Accel-Brain-Books/In-house_R_and_D_in_the_era_of_democratization_of_AI/book_cover.jpg" width="160px" /></a>
<p>
『<a href="https://www.amazon.co.jp/dp/B08PV4ZQG5/ref=sr_1_1?dchild=1&qid=1607343553&s=digital-text&sr=1-1&text=%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BEAccel+Brain" target="_blank">「AIの民主化」時代の企業内研究開発:
深層学習の「実学」としての機能分析</a>』(Japanese)
</p>
</div>
<p><br /></p>
<div data-align="center">
<a href="https://www.amazon.co.jp/dp/B093Z533LK" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Accel-Brain-Books/AI_vs_Investors_as_Noise_Traders/book_cover.jpg" width="160px" /></a>
<p>
『<a href="https://www.amazon.co.jp/dp/B093Z533LK" target="_blank">AI
vs. ノイズトレーダーとしての投資家たち:
「アルゴリズム戦争」時代の証券投資戦略</a>』(Japanese)
</p>
</div>
<p><br /></p>
<div data-align="center">
<a href="https://www.amazon.co.jp/dp/B0994CH3CM" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Accel-Brain-Books/Babel_of_Natural_Language_Processing/book_cover.jpg" width="160px" /></a>
<p>
『<a href="https://www.amazon.co.jp/dp/B0994CH3CM" target="_blank">自然言語処理のバベル:
文書自動要約、文章生成AI、チャットボットの意味論</a>』(Japanese)
</p>
</div>
<p><br /></p>
<div data-align="center">
<a href="https://www.amazon.co.jp/dp/B09C4KYZBX" target="_blank"><img src="https://storage.googleapis.com/accel-brain-code/Accel-Brain-Books/Origin_of_the_statistical_machine_learning/book_cover.jpg" width="160px" /></a>
<p>
『<a href="https://www.amazon.co.jp/dp/B09C4KYZBX" target="_blank">統計的機械学習の根源:
熱力学、量子力学、統計力学における天才物理学者たちの神学的な理念</a>』(Japanese)
</p>
</div>
<p>Specific references are the following papers and books.</p>
<h3 id="q-learning-models.">Q-Learning models.</h3>
<ul>
<li>Agrawal, S., &amp; Goyal, N. (2011). Analysis of Thompson sampling
for the multi-armed bandit problem. arXiv preprint arXiv:1111.1797.</li>
<li>Bubeck, S., &amp; Cesa-Bianchi, N. (2012). Regret analysis of
stochastic and nonstochastic multi-armed bandit problems. arXiv preprint
arXiv:1204.5721.</li>
<li>Chapelle, O., &amp; Li, L. (2011). An empirical evaluation of
thompson sampling. In Advances in neural information processing systems
(pp. 2249-2257).</li>
<li>Du, K. L., &amp; Swamy, M. N. S. (2016). Search and optimization by
metaheuristics (p. 434). New York City: Springer.</li>
<li>Kaufmann, E., Cappe, O., &amp; Garivier, A. (2012). On Bayesian
upper confidence bounds for bandit problems. In International Conference
on Artificial Intelligence and Statistics (pp. 592-600).</li>
<li>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
Wierstra, D., &amp; Riedmiller, M. (2013). Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602.</li>
<li>Richard Sutton and Andrew Barto (1998). Reinforcement Learning. MIT
Press.</li>
<li>Watkins, C. J. C. H. (1989). Learning from delayed rewards (Doctoral
dissertation, University of Cambridge).</li>
<li>Watkins, C. J., &amp; Dayan, P. (1992). Q-learning. Machine
learning, 8(3-4), 279-292.</li>
<li>White, J. (2012). Bandit algorithms for website optimization. ”
O’Reilly Media, Inc.”.</li>
</ul>
<h3 id="deep-q-network-models.">Deep Q-Network models.</h3>
<ul>
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares,
F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase
representations using RNN encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078.</li>
<li><a href="https://pdfs.semanticscholar.org/dd98/9d94613f439c05725bad958929357e365084.pdf" target="_blank">Egorov,
M. (2016). Multi-agent deep reinforcement learning.</a></li>
<li>Gupta, J. K., Egorov, M., &amp; Kochenderfer, M. (2017, May).
Cooperative multi-agent control using deep reinforcement learning. In
International Conference on Autonomous Agents and Multiagent Systems
(pp. 66-83). Springer, Cham.</li>
<li>Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P.,
&amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor
anomaly detection. arXiv preprint arXiv:1607.00148.</li>
<li>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
Wierstra, D., &amp; Riedmiller, M. (2013). Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602.</li>
<li>Sainath, T. N., Vinyals, O., Senior, A., &amp; Sak, H. (2015,
April). Convolutional, long short-term memory, fully connected deep
neural networks. In Acoustics, Speech and Signal Processing (ICASSP),
2015 IEEE International Conference on (pp. 4580-4584). IEEE.</li>
<li>Xingjian, S. H. I., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K.,
&amp; Woo, W. C. (2015). Convolutional LSTM network: A machine learning
approach for precipitation nowcasting. In Advances in neural information
processing systems (pp. 802-810).</li>
<li>Zaremba, W., Sutskever, I., &amp; Vinyals, O. (2014). Recurrent
neural network regularization. arXiv preprint arXiv:1409.2329.</li>
</ul>
<h3 id="annealing-models.">Annealing models.</h3>
<ul>
<li>Bektas, T. (2006). The multiple traveling salesman problem: an
overview of formulations and solution procedures. Omega, 34(3),
209-219.</li>
<li>Bertsimas, D., &amp; Tsitsiklis, J. (1993). Simulated annealing.
Statistical science, 8(1), 10-15.</li>
<li>Das, A., &amp; Chakrabarti, B. K. (Eds.). (2005). Quantum annealing
and related optimization methods (Vol. 679). Springer Science &amp;
Business Media.</li>
<li>Du, K. L., &amp; Swamy, M. N. S. (2016). Search and optimization by
metaheuristics. New York City: Springer.</li>
<li>Edwards, S. F., &amp; Anderson, P. W. (1975). Theory of spin
glasses. Journal of Physics F: Metal Physics, 5(5), 965.</li>
<li>Facchi, P., &amp; Pascazio, S. (2008). Quantum Zeno dynamics:
mathematical and physical aspects. Journal of Physics A: Mathematical
and Theoretical, 41(49), 493001.</li>
<li>Heim, B., Rønnow, T. F., Isakov, S. V., &amp; Troyer, M. (2015).
Quantum versus classical annealing of Ising spin glasses. Science,
348(6231), 215-217.</li>
<li>Heisenberg, W. (1925) Über quantentheoretische Umdeutung
kinematischer und mechanischer Beziehungen. Z. Phys. 33,
pp.879—893.</li>
<li>Heisenberg, W. (1927). Über den anschaulichen Inhalt der
quantentheoretischen Kinematik und Mechanik. Zeitschrift fur Physik, 43,
172-198.</li>
<li>Heisenberg, W. (1984). The development of quantum mechanics. In
Scientific Review Papers, Talks, and Books -Wissenschaftliche
Übersichtsartikel, Vorträge und Bücher (pp. 226-237). Springer Berlin
Heidelberg. Hilgevoord, Jan and Uffink, Jos, “The Uncertainty
Principle”, The Stanford Encyclopedia of Philosophy (Winter 2016
Edition), Edward N. Zalta (ed.), URL =
＜https://plato.stanford.edu/archives/win2016/entries/qt-uncertainty/＞.</li>
<li>Jarzynski, C. (1997). Nonequilibrium equality for free energy
differences. Physical Review Letters, 78(14), 2690.</li>
<li>Messiah, A. (1966). Quantum mechanics. 2 (1966). North-Holland
Publishing Company.</li>
<li>Mezard, M., &amp; Montanari, A. (2009). Information, physics, and
computation. Oxford University Press.</li>
<li>Nallusamy, R., Duraiswamy, K., Dhanalaksmi, R., &amp; Parthiban, P.
(2009). Optimization of non-linear multiple traveling salesman problem
using k-means clustering, shrink wrap algorithm and meta-heuristics.
International Journal of Nonlinear Science, 8(4), 480-487.</li>
<li>Schrödinger, E. (1926). Quantisierung als eigenwertproblem. Annalen
der physik, 385(13), S.437-490.</li>
<li>Somma, R. D., Batista, C. D., &amp; Ortiz, G. (2007). Quantum
approach to classical statistical mechanics. Physical review letters,
99(3), 030603.</li>
<li>鈴木正. (2008). 「組み合わせ最適化問題と量子アニーリング:
量子断熱発展の理論と性能評価」.,『物性研究』, 90(4): pp598-676.
参照箇所はpp619-624.</li>
<li>西森秀稔、大関真之(2018) 『量子アニーリングの基礎』須藤 彰三、岡 真
監修、共立出版、参照箇所はpp9-46.</li>
</ul>
<h2 id="author">Author</h2>
<ul>
<li>accel-brain</li>
</ul>
<h2 id="author-uri">Author URI</h2>
<ul>
<li>https://accel-brain.co.jp/</li>
<li>https://accel-brain.com/</li>
</ul>
<h2 id="license">License</h2>
<ul>
<li>GNU General Public License v2.0</li>
</ul>
