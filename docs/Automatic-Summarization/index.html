

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Automatic Summarization Library: pysummarization &#8212; pysummarization  documentation</title>
    <link rel="stylesheet" href="_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pysummarization package" href="pysummarization.html" />
    <link rel="prev" title="Welcome to pysummarization’s documentation!" href="index.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pysummarization.html" title="pysummarization package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="Welcome to pysummarization’s documentation!"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pysummarization  documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter">Welcome to pysummarization’s documentation!</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pysummarization.html"
                        title="next chapter">pysummarization package</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="automatic-summarization-library-pysummarization">
<span id="automatic-summarization-library-pysummarization"></span><h1>Automatic Summarization Library: pysummarization<a class="headerlink" href="#automatic-summarization-library-pysummarization" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">pysummarization</span></code> is Python3 library for the automatic summarization, document abstraction, and text filtering.</p>
<div class="section" id="description">
<span id="description"></span><h2>Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h2>
<p>The function of this library is automatic summarization using a kind of natural language processing and neural network language model. This library enable you to create a summary with the major points of the original document or web-scraped text that filtered by text clustering. And this library applies <a class="reference external" href="https://github.com/accel-brain/accel-brain-code/tree/master/Accel-Brain-Base">accel-brain-base</a> to implement <strong>Encoder/Decoder based on LSTM</strong> improving the accuracy of summarization by <strong>Sequence-to-Sequence</strong>(<strong>Seq2Seq</strong>) learning.</p>
</div>
<div class="section" id="documentation">
<span id="documentation"></span><h2>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<p>Full documentation is available on <a class="reference external" href="https://code.accel-brain.com/Automatic-Summarization/">https://code.accel-brain.com/Automatic-Summarization/</a> . This document contains information on functionally reusability, functional scalability and functional extensibility.</p>
</div>
<div class="section" id="installation">
<span id="installation"></span><h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Install using pip:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install pysummarization
</pre></div>
</div>
<div class="section" id="source-code">
<span id="source-code"></span><h3>Source code<a class="headerlink" href="#source-code" title="Permalink to this headline">¶</a></h3>
<p>The source code is currently hosted on GitHub.</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/chimera0/accel-brain-code/tree/master/Automatic-Summarization">accel-brain-code/Automatic-Summarization</a></li>
</ul>
</div>
<div class="section" id="python-package-index-pypi">
<span id="python-package-index-pypi"></span><h3>Python package index(PyPI)<a class="headerlink" href="#python-package-index-pypi" title="Permalink to this headline">¶</a></h3>
<p>Installers for the latest released version are available at the Python package index.</p>
<ul class="simple">
<li><a class="reference external" href="https://pypi.python.org/pypi/pysummarization/">pysummarization : Python Package Index</a></li>
</ul>
</div>
<div class="section" id="dependencies">
<span id="dependencies"></span><h3>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/numpy/numpy">numpy</a>: v1.13.3 or higher.</li>
<li><a class="reference external" href="https://github.com/nltk/nltk">nltk</a>: v3.2.3 or higher.</li>
</ul>
<div class="section" id="options">
<span id="options"></span><h4>Options<a class="headerlink" href="#options" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="https://github.com/SamuraiT/mecab-python3">mecab-python3</a>: v0.7 or higher.<ul>
<li>Relevant only for Japanese.</li>
</ul>
</li>
<li><a class="reference external" href="https://github.com/brechin/pdfminer2">pdfminer2</a>(or <a class="reference external" href="https://github.com/pdfminer/pdfminer.six">pdfminer.six</a>): latest.<ul>
<li>Relevant only for PDF files.</li>
</ul>
</li>
<li><a class="reference external" href="https://github.com/gawel/pyquery">pyquery</a>:v1.2.17 or higher.<ul>
<li>Relevant only for web scraiping.</li>
</ul>
</li>
<li><a class="reference external" href="https://github.com/accel-brain/accel-brain-code/tree/master/Accel-Brain-Base">accel-brain-base</a>: v1.0.0 or higher.<ul>
<li>Only when using <strong>Re-Seq2Seq</strong> and <strong>EncDec-AD</strong>.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="usecase-summarize-an-english-string-argument">
<span id="usecase-summarize-an-english-string-argument"></span><h2>Usecase: Summarize an English string argument.<a class="headerlink" href="#usecase-summarize-an-english-string-argument" title="Permalink to this headline">¶</a></h2>
<p>Import Python modules.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.nlpbase.auto_abstractor</span> <span class="kn">import</span> <span class="n">AutoAbstractor</span>
<span class="kn">from</span> <span class="nn">pysummarization.tokenizabledoc.simple_tokenizer</span> <span class="kn">import</span> <span class="n">SimpleTokenizer</span>
<span class="kn">from</span> <span class="nn">pysummarization.abstractabledoc.top_n_rank_abstractor</span> <span class="kn">import</span> <span class="n">TopNRankAbstractor</span>
</pre></div>
</div>
<p>Prepare an English string argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;Natural language generation (NLG) is the natural language processing task of generating natural language from a machine representation system such as a knowledge base or a logical form. Psycholinguists prefer the term language production when such formal representations are interpreted as models for mental representations.&quot;</span>
</pre></div>
</div>
<p>And instantiate objects and call the method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Object of automatic summarization.</span>
<span class="n">auto_abstractor</span> <span class="o">=</span> <span class="n">AutoAbstractor</span><span class="p">()</span>
<span class="c1"># Set tokenizer.</span>
<span class="n">auto_abstractor</span><span class="o">.</span><span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">()</span>
<span class="c1"># Set delimiter for making a list of sentence.</span>
<span class="n">auto_abstractor</span><span class="o">.</span><span class="n">delimiter_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
<span class="c1"># Object of abstracting and filtering document.</span>
<span class="n">abstractable_doc</span> <span class="o">=</span> <span class="n">TopNRankAbstractor</span><span class="p">()</span>
<span class="c1"># Summarize document.</span>
<span class="n">result_dict</span> <span class="o">=</span> <span class="n">auto_abstractor</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">abstractable_doc</span><span class="p">)</span>

<span class="c1"># Output result.</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">result_dict</span><span class="p">[</span><span class="s2">&quot;summarize_result&quot;</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">result_dict</span></code> is a dict. this format is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="nb">dict</span><span class="p">{</span>
     <span class="s2">&quot;summarize_result&quot;</span><span class="p">:</span> <span class="s2">&quot;The list of summarized sentences.&quot;</span><span class="p">,</span> 
     <span class="s2">&quot;scoring_data&quot;</span><span class="p">:</span>     <span class="s2">&quot;The list of scores(Rank of importance).&quot;</span>
 <span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="usecase-summarize-japanese-string-argument">
<span id="usecase-summarize-japanese-string-argument"></span><h2>Usecase: Summarize Japanese string argument.<a class="headerlink" href="#usecase-summarize-japanese-string-argument" title="Permalink to this headline">¶</a></h2>
<p>Import Python modules.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.nlpbase.auto_abstractor</span> <span class="kn">import</span> <span class="n">AutoAbstractor</span>
<span class="kn">from</span> <span class="nn">pysummarization.tokenizabledoc.mecab_tokenizer</span> <span class="kn">import</span> <span class="n">MeCabTokenizer</span>
<span class="kn">from</span> <span class="nn">pysummarization.abstractabledoc.top_n_rank_abstractor</span> <span class="kn">import</span> <span class="n">TopNRankAbstractor</span>
</pre></div>
</div>
<p>Prepare an English string argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;自然言語処理（しぜんげんごしょり、英語: natural language processing、略称：NLP）は、人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の一分野である。「計算言語学」（computational linguistics）との類似もあるが、自然言語処理は工学的な視点からの言語処理をさすのに対して、計算言語学は言語学的視点を重視する手法をさす事が多い[1]。データベース内の情報を自然言語に変換したり、自然言語の文章をより形式的な（コンピュータが理解しやすい）表現に変換するといった処理が含まれる。応用例としては予測変換、IMEなどの文字変換が挙げられる。&quot;</span>
</pre></div>
</div>
<p>And instantiate objects and call the method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Object of automatic summarization.</span>
<span class="n">auto_abstractor</span> <span class="o">=</span> <span class="n">AutoAbstractor</span><span class="p">()</span>
<span class="c1"># Set tokenizer for Japanese.</span>
<span class="n">auto_abstractor</span><span class="o">.</span><span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">MeCabTokenizer</span><span class="p">()</span>
<span class="c1"># Set delimiter for making a list of sentence.</span>
<span class="n">auto_abstractor</span><span class="o">.</span><span class="n">delimiter_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;。&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
<span class="c1"># Object of abstracting and filtering document.</span>
<span class="n">abstractable_doc</span> <span class="o">=</span> <span class="n">TopNRankAbstractor</span><span class="p">()</span>
<span class="c1"># Summarize document.</span>
<span class="n">result_dict</span> <span class="o">=</span> <span class="n">auto_abstractor</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">abstractable_doc</span><span class="p">)</span>

<span class="c1"># Output result.</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">result_dict</span><span class="p">[</span><span class="s2">&quot;summarize_result&quot;</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="usecase-english-web-page-summarization">
<span id="usecase-english-web-page-summarization"></span><h2>Usecase: English Web-Page Summarization<a class="headerlink" href="#usecase-english-web-page-summarization" title="Permalink to this headline">¶</a></h2>
<p>Run the batch program: <a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Automatic-Summarization/demo/demo_summarization_english_web_page.py">demo/demo_summarization_english_web_page.py</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_summarization_english_web_page</span><span class="o">.</span><span class="n">py</span> <span class="p">{</span><span class="n">URL</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>{URL}: web site URL.</li>
</ul>
<div class="section" id="demo">
<span id="demo"></span><h3>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h3>
<p>Let’s summarize this page: <a class="reference external" href="https://en.wikipedia.org/wiki/Natural_language_generation">Natural_language_generation - Wikipedia</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_summarization_english_web_page</span><span class="o">.</span><span class="n">py</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">en</span><span class="o">.</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">wiki</span><span class="o">/</span><span class="n">Natural_language_generation</span>
</pre></div>
</div>
<p>The result is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Natural</span> <span class="n">language</span> <span class="n">generation</span> <span class="n">From</span> <span class="n">Wikipedia</span><span class="p">,</span> <span class="n">the</span> <span class="n">free</span> <span class="n">encyclopedia</span> <span class="n">Jump</span> <span class="n">to</span><span class="p">:</span> <span class="n">navigation</span> <span class="p">,</span> <span class="n">search</span> <span class="n">Natural</span> <span class="n">language</span> <span class="n">generation</span> <span class="p">(</span> <span class="n">NLG</span> <span class="p">)</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span> <span class="n">task</span> <span class="n">of</span> <span class="n">generating</span> <span class="n">natural</span> <span class="n">language</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">machine</span> <span class="n">representation</span> <span class="n">system</span> <span class="n">such</span> <span class="k">as</span> <span class="n">a</span> <span class="n">knowledge</span> <span class="n">base</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">logical</span> <span class="n">form</span> <span class="o">.</span>

 <span class="n">Psycholinguists</span> <span class="n">prefer</span> <span class="n">the</span> <span class="n">term</span> <span class="n">language</span> <span class="n">production</span> <span class="n">when</span> <span class="n">such</span> <span class="n">formal</span> <span class="n">representations</span> <span class="n">are</span> <span class="n">interpreted</span> <span class="k">as</span> <span class="n">models</span> <span class="k">for</span> <span class="n">mental</span> <span class="n">representations</span><span class="o">.</span>

 <span class="n">It</span> <span class="n">could</span> <span class="n">be</span> <span class="n">said</span> <span class="n">an</span> <span class="n">NLG</span> <span class="n">system</span> <span class="ow">is</span> <span class="n">like</span> <span class="n">a</span> <span class="n">translator</span> <span class="n">that</span> <span class="n">converts</span> <span class="n">data</span> <span class="n">into</span> <span class="n">a</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">representation</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="usecase-japanese-web-page-summarization">
<span id="usecase-japanese-web-page-summarization"></span><h2>Usecase: Japanese Web-Page Summarization<a class="headerlink" href="#usecase-japanese-web-page-summarization" title="Permalink to this headline">¶</a></h2>
<p>Run the batch program: <a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Automatic-Summarization/demo/demo_summarization_japanese_web_page.py">demo/demo_summarization_japanese_web_page.py</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_summarization_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="p">{</span><span class="n">URL</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>{URL}: web site URL.</li>
</ul>
<div class="section" id="demo">
<span id="id1"></span><h3>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h3>
<p>Let’s summarize this page: <a class="reference external" href="https://ja.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E8%A6%81%E7%B4%84">自動要約 - Wikipedia</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_summarization_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ja</span><span class="o">.</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">wiki</span><span class="o">/%</span><span class="n">E8</span><span class="o">%</span><span class="mi">87</span><span class="o">%</span><span class="n">AA</span><span class="o">%</span><span class="n">E5</span><span class="o">%</span><span class="mi">8</span><span class="n">B</span><span class="o">%</span><span class="mi">95</span><span class="o">%</span><span class="n">E8</span><span class="o">%</span><span class="n">A6</span><span class="o">%</span><span class="mi">81</span><span class="o">%</span><span class="n">E7</span><span class="o">%</span><span class="n">B4</span><span class="o">%</span><span class="mi">84</span>
</pre></div>
</div>
<p>The result is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> 自動要約 （じどうようやく）は、 コンピュータプログラム を用いて、文書からその要約を作成する処理である。

自動要約の応用先の1つは Google などの 検索エンジン であるが、もちろん独立した1つの要約プログラムといったものもありうる。

 単一文書要約と複数文書要約 [ 編集 ] 単一文書要約 は、単一の文書を要約の対象とするものである。

例えば、1つの新聞記事を要約する作業は単一文書要約である。
</pre></div>
</div>
</div>
</div>
<div class="section" id="usecase-japanese-web-page-summarization-with-n-gram">
<span id="usecase-japanese-web-page-summarization-with-n-gram"></span><h2>Usecase: Japanese Web-Page Summarization with N-gram<a class="headerlink" href="#usecase-japanese-web-page-summarization-with-n-gram" title="Permalink to this headline">¶</a></h2>
<p>The minimum unit of token is not necessarily <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">word</span></code> in automatic summarization. <code class="docutils literal notranslate"><span class="pre">N-gram</span></code> is also applicable to the tokenization.</p>
<p>Run the batch program: <a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Automatic-Summarization/demo/demo_with_n_gram_japanese_web_page.py">demo/demo_with_n_gram_japanese_web_page.py</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo_with_n_gram_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="p">{</span><span class="n">URL</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>{URL}: web site URL.</li>
</ul>
<div class="section" id="demo">
<span id="id2"></span><h3>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h3>
<p>Let’s summarize this page:<a class="reference external" href="https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2">情報検索 - Wikipedia</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_with_n_gram_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ja</span><span class="o">.</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">wiki</span><span class="o">/%</span><span class="n">E6</span><span class="o">%</span><span class="mi">83</span><span class="o">%</span><span class="mi">85</span><span class="o">%</span><span class="n">E5</span><span class="o">%</span><span class="n">A0</span><span class="o">%</span><span class="n">B1</span><span class="o">%</span><span class="n">E6</span><span class="o">%</span><span class="n">A4</span><span class="o">%</span><span class="mi">9</span><span class="n">C</span><span class="o">%</span><span class="n">E7</span><span class="o">%</span><span class="n">B4</span><span class="o">%</span><span class="n">A2</span>
</pre></div>
</div>
<p>The result is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>情報検索アルゴリズムの詳細については 情報検索アルゴリズム を参照のこと。

 パターンマッチング 検索質問として入力された表現をそのまま含む文書を検索するアルゴリズム。

 ベクトル空間モデル キーワード等を各 次元 として設定した高次元 ベクトル空間 を想定し、検索の対象とするデータやユーザによる検索質問に何らかの加工を行い ベクトル を生成する
</pre></div>
</div>
</div>
</div>
<div class="section" id="usecase-summarization-filtering-the-mutually-similar-tautological-pleonastic-or-redundant-sentences">
<span id="usecase-summarization-filtering-the-mutually-similar-tautological-pleonastic-or-redundant-sentences"></span><h2>Usecase: Summarization, filtering the mutually similar, tautological, pleonastic, or redundant sentences<a class="headerlink" href="#usecase-summarization-filtering-the-mutually-similar-tautological-pleonastic-or-redundant-sentences" title="Permalink to this headline">¶</a></h2>
<p>If the sentences you want to summarize consist of repetition of same or similar sense in different words, the summary results may also be redundant. Then before summarization, you should filter the mutually similar, tautological, pleonastic, or redundant sentences to extract features having an information quantity. The function of <code class="docutils literal notranslate"><span class="pre">SimilarityFilter</span></code> is to cut-off the sentences having the state of resembling or being alike by calculating the similarity measure.</p>
<p>But there is no reason to stick to a single similarity concept. <em>Modal logically</em>, the definition of this concept is <em>contingent</em>, like the concept of <em>distance</em>. Even if one similarity or distance function is defined in relation to a problem setting, there are always <em>functionally equivalent</em> algorithms to solve the problem setting. Then this library has a wide variety of subtyping polymorphisms of <code class="docutils literal notranslate"><span class="pre">SimilarityFilter</span></code>.</p>
<div class="section" id="dice-jaccard-and-simpson">
<span id="dice-jaccard-and-simpson"></span><h3>Dice, Jaccard, and Simpson<a class="headerlink" href="#dice-jaccard-and-simpson" title="Permalink to this headline">¶</a></h3>
<p>There are some classes for calculating the similarity measure. In this library, <strong>Dice coefficient</strong>, <strong>Jaccard coefficient</strong>, and <strong>Simpson coefficient</strong> between two sentences is calculated as follows.</p>
<p>Import Python modules for calculating the similarity measure and instantiate the object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.similarityfilter.dice</span> <span class="kn">import</span> <span class="n">Dice</span>
<span class="n">similarity_filter</span> <span class="o">=</span> <span class="n">Dice</span><span class="p">()</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.similarityfilter.jaccard</span> <span class="kn">import</span> <span class="n">Jaccard</span>
<span class="n">similarity_filter</span> <span class="o">=</span> <span class="n">Jaccard</span><span class="p">()</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.similarityfilter.simpson</span> <span class="kn">import</span> <span class="n">Simpson</span>
<span class="n">similarity_filter</span> <span class="o">=</span> <span class="n">Simpson</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="functional-equivalent-combination-of-tf-idf-and-cosine-similarity">
<span id="functional-equivalent-combination-of-tf-idf-and-cosine-similarity"></span><h3>Functional equivalent: Combination of Tf-Idf and Cosine similarity<a class="headerlink" href="#functional-equivalent-combination-of-tf-idf-and-cosine-similarity" title="Permalink to this headline">¶</a></h3>
<p>If you want to calculate similarity with <strong>Tf-Idf cosine similarity</strong>, instantiate <code class="docutils literal notranslate"><span class="pre">TfIdfCosine</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.similarityfilter.tfidf_cosine</span> <span class="kn">import</span> <span class="n">TfIdfCosine</span>
<span class="n">similarity_filter</span> <span class="o">=</span> <span class="n">TfIdfCosine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="calculating-similarity">
<span id="calculating-similarity"></span><h3>Calculating similarity<a class="headerlink" href="#calculating-similarity" title="Permalink to this headline">¶</a></h3>
<p>If you want to calculate similarity between two sentences, call <code class="docutils literal notranslate"><span class="pre">calculate</span></code> method as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tokenized sentences</span>
<span class="n">token_list_x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Dice&quot;</span><span class="p">,</span> <span class="s2">&quot;coefficient&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;similarity&quot;</span><span class="p">,</span> <span class="s2">&quot;measure&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">]</span>
<span class="n">token_list_y</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Jaccard&quot;</span><span class="p">,</span> <span class="s2">&quot;coefficient&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;similarity&quot;</span><span class="p">,</span> <span class="s2">&quot;measure&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">]</span>
<span class="c1"># 0.75</span>
<span class="n">similarity_num</span> <span class="o">=</span> <span class="n">similarity_filter</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">token_list_x</span><span class="p">,</span> <span class="n">token_list_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="filtering-similar-sentences-and-summarization">
<span id="filtering-similar-sentences-and-summarization"></span><h3>Filtering similar sentences and summarization<a class="headerlink" href="#filtering-similar-sentences-and-summarization" title="Permalink to this headline">¶</a></h3>
<p>The function of these methods is to cut-off mutually similar sentences. In text summarization, basic usage of this function is as follow. After all, <code class="docutils literal notranslate"><span class="pre">SimilarityFilter</span></code> is delegated as well as GoF’s Strategy Pattern.</p>
<p>Import Python modules for NLP and text summarization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.nlp_base</span> <span class="kn">import</span> <span class="n">NlpBase</span>
<span class="kn">from</span> <span class="nn">pysummarization.nlpbase.auto_abstractor</span> <span class="kn">import</span> <span class="n">AutoAbstractor</span>
<span class="kn">from</span> <span class="nn">pysummarization.tokenizabledoc.mecab_tokenizer</span> <span class="kn">import</span> <span class="n">MeCabTokenizer</span>
<span class="kn">from</span> <span class="nn">pysummarization.abstractabledoc.top_n_rank_abstractor</span> <span class="kn">import</span> <span class="n">TopNRankAbstractor</span>
<span class="kn">from</span> <span class="nn">pysummarization.similarityfilter.tfidf_cosine</span> <span class="kn">import</span> <span class="n">TfIdfCosine</span>
</pre></div>
</div>
<p>Instantiate object of the NLP.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The object of the NLP.</span>
<span class="n">nlp_base</span> <span class="o">=</span> <span class="n">NlpBase</span><span class="p">()</span>
<span class="c1"># Set tokenizer. This is japanese tokenizer with MeCab.</span>
<span class="n">nlp_base</span><span class="o">.</span><span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">MeCabTokenizer</span><span class="p">()</span>
</pre></div>
</div>
<p>Instantiate object of <code class="docutils literal notranslate"><span class="pre">SimilarityFilter</span></code> and set the cut-off threshold.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The object of `Similarity Filter`. </span>
<span class="c1"># The similarity observed by this object is so-called cosine similarity of Tf-Idf vectors.</span>
<span class="n">similarity_filter</span> <span class="o">=</span> <span class="n">TfIdfCosine</span><span class="p">()</span>

<span class="c1"># Set the object of NLP.</span>
<span class="n">similarity_filter</span><span class="o">.</span><span class="n">nlp_base</span> <span class="o">=</span> <span class="n">nlp_base</span>

<span class="c1"># If the similarity exceeds this value, the sentence will be cut off.</span>
<span class="n">similarity_filter</span><span class="o">.</span><span class="n">similarity_limit</span> <span class="o">=</span> <span class="mf">0.25</span>
</pre></div>
</div>
<p>Prepare sentences you want to summarize.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Summarized sentences (sited from http://ja.uncyclopedia.info/wiki/%E5%86%97%E8%AA%9E%E6%B3%95).</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;冗語法（じょうごほう、レデュンダンシー、redundancy、jōgohō）とは、何度も何度も繰り返し重ねて重複して前述されたのと同じ意味の同様である同意義の文章を、必要あるいは説明か理解を要求された以上か、伝え伝達したいと意図された、あるいは表し表現したい意味以上に、繰り返し重ねて重複して繰り返すことによる、不必要であるか、または余分な余計である文章の、必要以上の使用であり、何度も何度も繰り返し重ねて重複して前述されたのと同じ意味の同様の文章を、必要あるいは説明か理解を要求された以上か、伝え伝達したいと意図された、あるいは表し表現したい意味以上に、繰り返し重ねて重複して繰り返すことによる、不必要であるか、または余分な文章の、必要以上の使用である。これが冗語法（じょうごほう、レデュンダンシー、redundancy、jōgohō）である。基本的に、冗語法（じょうごほう、レデュンダンシー、redundancy、jōgohō）が多くの場合において概して一般的に繰り返される通常の場合は、普通、同じ同様の発想や思考や概念や物事を表し表現する別々の異なった文章や単語や言葉が何回も何度も余分に繰り返され、その結果として発言者の考えが何回も何度も言い直され、事実上、実際に同じ同様の発言が何回も何度にもわたり、幾重にも言い換えられ、かつ、同じことが何回も何度も繰り返し重複して過剰に回数を重ね前述されたのと同じ意味の同様の文章が何度も何度も不必要に繰り返される。通常の場合、多くの場合において概して一般的にこのように冗語法（じょうごほう、レデュンダンシー、redundancy、jōgohō）が繰り返される。&quot;</span>
</pre></div>
</div>
<p>Instantiate object of <code class="docutils literal notranslate"><span class="pre">AutoAbstractor</span></code> and call the method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The object of automatic sumamrization.</span>
<span class="n">auto_abstractor</span> <span class="o">=</span> <span class="n">AutoAbstractor</span><span class="p">()</span>
<span class="c1"># Set tokenizer. This is japanese tokenizer with MeCab.</span>
<span class="n">auto_abstractor</span><span class="o">.</span><span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">MeCabTokenizer</span><span class="p">()</span>
<span class="c1"># Object of abstracting and filtering document.</span>
<span class="n">abstractable_doc</span> <span class="o">=</span> <span class="n">TopNRankAbstractor</span><span class="p">()</span>
<span class="c1"># Delegate the objects and execute summarization.</span>
<span class="n">result_dict</span> <span class="o">=</span> <span class="n">auto_abstractor</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">abstractable_doc</span><span class="p">,</span> <span class="n">similarity_filter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="demo">
<span id="id3"></span><h3>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h3>
<p>Let’s summarize this page:<a class="reference external" href="https://ja.wikipedia.org/wiki/%E5%BE%AA%E7%92%B0%E8%AB%96%E6%B3%95">循環論法 - Wikipedia</a>.</p>
<p>Run the batch program: <a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Automatic-Summarization/demo/demo_similarity_filtering_japanese_web_page.py">demo/demo_similarity_filtering_japanese_web_page.py</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_similarity_filtering_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="p">{</span><span class="n">URL</span><span class="p">}</span> <span class="p">{</span><span class="n">SimilarityFilter</span><span class="p">}</span> <span class="p">{</span><span class="n">SimilarityLimit</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>{URL}: web site URL.</li>
<li>{SimilarityFilter}: The object of <code class="docutils literal notranslate"><span class="pre">SimilarityFilter</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">Dice</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">Jaccard</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">Simpson</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">TfIdfCosine</span></code></li>
</ul>
</li>
<li>{SimilarityLimit}: The cut-off threshold.</li>
</ul>
<p>For instance, command line argument is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">demo</span><span class="o">/</span><span class="n">demo_similarity_filtering_japanese_web_page</span><span class="o">.</span><span class="n">py</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ja</span><span class="o">.</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">wiki</span><span class="o">/%</span><span class="n">E5</span><span class="o">%</span><span class="n">BE</span><span class="o">%</span><span class="n">AA</span><span class="o">%</span><span class="n">E7</span><span class="o">%</span><span class="mi">92</span><span class="o">%</span><span class="n">B0</span><span class="o">%</span><span class="n">E8</span><span class="o">%</span><span class="n">AB</span><span class="o">%</span><span class="mi">96</span><span class="o">%</span><span class="n">E6</span><span class="o">%</span><span class="n">B3</span><span class="o">%</span><span class="mi">95</span> <span class="n">Jaccard</span> <span class="mf">0.3</span>
</pre></div>
</div>
<p>The result is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>循環論法 出典: フリー百科事典『ウィキペディア（Wikipedia）』 移動先: 案内 、 検索 循環論法 （じゅんかんろんぽう、circular reasoning, circular logic, vicious circle [1] ）とは、 ある命題の 証明 において、その命題を仮定した議論を用いること [1] 。

証明すべき結論を前提として用いる論法 [2] 。

 ある用語の 定義 を与える表現の中にその用語自体が本質的に登場していること [1]
</pre></div>
</div>
</div>
</div>
<div class="section" id="usecase-summarization-with-neural-network-language-model">
<span id="usecase-summarization-with-neural-network-language-model"></span><h2>Usecase: Summarization with Neural Network Language Model.<a class="headerlink" href="#usecase-summarization-with-neural-network-language-model" title="Permalink to this headline">¶</a></h2>
<p>According to the neural networks theory, and in relation to manifold hypothesis, it is well known that multilayer neural networks can learn features of observed data points and have the feature points in hidden layer. High-dimensional data can be converted to low-dimensional codes by training the model such as <strong>Stacked Auto-Encoder</strong> and <strong>Encoder/Decoder</strong> with a small central layer to reconstruct high-dimensional input vectors. This function of dimensionality reduction facilitates feature expressions to calculate similarity of each data point.</p>
<p>This library provides <strong>Encoder/Decoder based on LSTM</strong>, which makes it possible to extract series features of natural sentences embedded in deeper layers by <strong>sequence-to-sequence learning</strong>. <em>Intuitively</em> speaking, similarities of the series feature points correspond to similarities of the observed data points. If we believe this hypothesis, the following models become in principle possible.</p>
<div class="section" id="retrospective-sequence-to-sequence-learning-re-seq2seq">
<span id="retrospective-sequence-to-sequence-learning-re-seq2seq"></span><h3>retrospective sequence-to-sequence learning(re-seq2seq).<a class="headerlink" href="#retrospective-sequence-to-sequence-learning-re-seq2seq" title="Permalink to this headline">¶</a></h3>
<p>The concept of the re-seq2seq(Zhang, K. et al., 2018) provided inspiration to this library. This model is a new sequence learning model mainly in the field of Video Summarizations. “The key idea behind re-seq2seq is to measure how well the machine-generated summary is similar to the original video in an abstract semantic space” (Zhang, K. et al., 2018, p3).</p>
<p>The encoder of a seq2seq model observes the original video and output feature points which represents the semantic meaning of the observed data points. Then the feature points is observed by the decoder of this model. Additionally, in the re-seq2seq model, the outputs of the decoder is propagated to a retrospective encoder, which infers feature points to represent the semantic meaning of the summary. “If the summary preserves the important and relevant information in the original video, then we should expect that the  two embeddings are similar (e.g. in Euclidean distance)” (Zhang, K. et al., 2018, p3).</p>
<div>
<img src="https://storage.googleapis.com/accel-brain-code/Automatic-Summarization/img/re-seq-2-seq-semantics.png">
<p>Zhang, K., Grauman, K., & Sha, F. (2018). Retrospective Encoders for Video Summarization. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 383-399), p2.</p>
</div><p>This library refers to this intuitive insight above to apply the model to text summarizations. Like videos, semantic feature representation based on representation learning of manifolds is also possible in text summarizations.</p>
<p>The intuition in the design of their loss function is also suggestive. “The intuition behind our modeling is that the outputs should convey the same amount of information as the inputs. For summarization, this is precisely the goal: a good summary should be such that after viewing the summary, users would get about the same amount of information as if they had viewed the original video” (Zhang, K. et al., 2018, p7).</p>
<div class="section" id="building-retrospective-sequence-to-sequence-learning-re-seq2seq">
<span id="building-retrospective-sequence-to-sequence-learning-re-seq2seq"></span><h4>Building retrospective sequence-to-sequence learning(re-seq2seq).<a class="headerlink" href="#building-retrospective-sequence-to-sequence-learning-re-seq2seq" title="Permalink to this headline">¶</a></h4>
<p>Import Python modules.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.abstractablesemantics._mxnet.re_seq_2_seq</span> <span class="kn">import</span> <span class="n">ReSeq2Seq</span>
<span class="kn">from</span> <span class="nn">pysummarization.iteratabledata._mxnet.token_iterator</span> <span class="kn">import</span> <span class="n">TokenIterator</span>
<span class="kn">from</span> <span class="nn">pysummarization.nlp_base</span> <span class="kn">import</span> <span class="n">NlpBase</span>
<span class="kn">from</span> <span class="nn">pysummarization.tokenizabledoc.simple_tokenizer</span> <span class="kn">import</span> <span class="n">SimpleTokenizer</span>
<span class="kn">from</span> <span class="nn">pysummarization.vectorizabletoken.t_hot_vectorizer</span> <span class="kn">import</span> <span class="n">THotVectorizer</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
</pre></div>
</div>
<p>Setup a logger.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span><span class="p">,</span> <span class="n">StreamHandler</span><span class="p">,</span> <span class="n">NullHandler</span><span class="p">,</span> <span class="n">DEBUG</span><span class="p">,</span> <span class="n">ERROR</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;accelbrainbase&quot;</span><span class="p">)</span>
<span class="n">handler</span> <span class="o">=</span> <span class="n">StreamHandler</span><span class="p">()</span>
<span class="n">handler</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
</pre></div>
</div>
<p>Initialize a tokenizer and a vectorizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># `str` of your document.</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;Your document.&quot;</span>

<span class="n">nlp_base</span> <span class="o">=</span> <span class="n">NlpBase</span><span class="p">()</span>
<span class="n">nlp_base</span><span class="o">.</span><span class="n">delimiter_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
<span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">()</span>
<span class="n">sentence_list</span> <span class="o">=</span> <span class="n">nlp_base</span><span class="o">.</span><span class="n">listup_sentence</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizable_doc</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
</pre></div>
</div>
<p>Setup the vectorizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizable_token</span> <span class="o">=</span> <span class="n">THotVectorizer</span><span class="p">(</span><span class="n">token_list</span><span class="o">=</span><span class="n">token_arr</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="n">vectorizable_token</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">token_list</span><span class="o">=</span><span class="n">token_arr</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">vector_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vector_list</span><span class="p">)</span>
<span class="n">token_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">token_list</span><span class="p">)</span>

<span class="n">token_iterator</span> <span class="o">=</span> <span class="n">TokenIterator</span><span class="p">(</span>
    <span class="n">vectorizable_token</span><span class="o">=</span><span class="n">vectorizable_token</span><span class="p">,</span> 
    <span class="n">token_arr</span><span class="o">=</span><span class="n">token_arr</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">norm_mode</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ctx</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">observed_arr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">token_iterator</span><span class="o">.</span><span class="n">generate_learned_samples</span><span class="p">():</span>
    <span class="k">break</span>
<span class="k">print</span><span class="p">(</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch size, the length of series, dimension)</span>
</pre></div>
</div>
<p>Instantiate <code class="docutils literal notranslate"><span class="pre">ReSeq2Seq</span></code> and input hyperparameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstractable_semantics</span> <span class="o">=</span> <span class="n">ReSeq2Seq</span><span class="p">(</span>
    <span class="c1"># The default parameter. The number of units in hidden layers.</span>
    <span class="n">hidden_neuron_count</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="c1"># The default parameter. The number of units in output layer.</span>
    <span class="n">output_neuron_count</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="c1"># The rate of dropout.</span>
    <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="c1"># Batch size.</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="c1"># Learning rate.</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
    <span class="c1"># The length of series.</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="c1"># `mx.gpu()` or `mx.cpu()`.</span>
    <span class="n">ctx</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Execute <code class="docutils literal notranslate"><span class="pre">learn</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstractable_semantics</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span>
    <span class="c1"># is-a `TokenIterator`.</span>
    <span class="n">token_iterator</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Execute <code class="docutils literal notranslate"><span class="pre">summarize</span></code> method to extract summaries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstract_list</span> <span class="o">=</span> <span class="n">abstractable_semantics</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span>
    <span class="c1"># is-a `TokenIterator`.</span>
    <span class="n">token_iterator</span><span class="p">,</span>
    <span class="c1"># is-a `VectorizableToken`.</span>
    <span class="n">vectorizable_token</span><span class="p">,</span>
    <span class="c1"># `list` of `str`, extracted by `nlp_base.listup_sentence(document)`.</span>
    <span class="n">sentence_list</span><span class="p">,</span>
    <span class="c1"># The number of extracted sentences.</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">abstract_list</span></code> is a <code class="docutils literal notranslate"><span class="pre">list</span></code> that contains <code class="docutils literal notranslate"><span class="pre">str</span></code>s of sentences.</p>
</div>
</div>
<div class="section" id="functional-equivalent-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad">
<span id="functional-equivalent-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad"></span><h3>Functional equivalent: LSTM-based Encoder/Decoder scheme for Anomaly Detection (EncDec-AD).<a class="headerlink" href="#functional-equivalent-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad" title="Permalink to this headline">¶</a></h3>
<p>This library applies the Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) to text summarizations by intuition. In this scheme, LSTM-based Encoder/Decoder or so-called the sequence-to-sequence(Seq2Seq) model learns to reconstruct normal time-series behavior, and thereafter uses reconstruction error to detect anomalies.</p>
<p>Malhotra, P., et al. (2016) showed that EncDecAD paradigm is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, they showed that the paradigm is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).</p>
<div><img src="https://storage.googleapis.com/accel-brain-code/Deep-Learning-by-means-of-Design-Pattern/img/latex/encoder_decoder.png" />
<p>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078., p2.</p>
</div><p>This library refers to the intuitive insight in relation to the use case of reconstruction error to detect anomalies above to apply the model to text summarization. As exemplified by Seq2Seq paradigm, document and sentence which contain tokens of text can be considered as time-series features. The anomalies data detected by EncDec-AD should have to express something about the text.</p>
<p>From the above analogy, this library introduces two conflicting intuitions. On the one hand, the anomalies data may catch observer’s eye from the viewpoints of rarity or amount of information as the indicator of natural language processing like TF-IDF shows. On the other hand, the anomalies data may be ignorable noise as mere outlier.</p>
<p>In any case, this library deduces the function and potential of EncDec-AD in text summarization is to draw the distinction of normal and anomaly texts and is to filter the one from the other.</p>
<div class="section" id="building-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad">
<span id="building-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad"></span><h4>Building LSTM-based Encoder/Decoder scheme for Anomaly Detection (EncDec-AD).<a class="headerlink" href="#building-lstm-based-encoder-decoder-scheme-for-anomaly-detection-encdec-ad" title="Permalink to this headline">¶</a></h4>
<p>Import Python modules.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysummarization.abstractablesemantics._mxnet.enc_dec_ad</span> <span class="kn">import</span> <span class="n">EncDecAD</span>
<span class="kn">from</span> <span class="nn">pysummarization.iteratabledata._mxnet.token_iterator</span> <span class="kn">import</span> <span class="n">TokenIterator</span>
<span class="kn">from</span> <span class="nn">pysummarization.nlp_base</span> <span class="kn">import</span> <span class="n">NlpBase</span>
<span class="kn">from</span> <span class="nn">pysummarization.tokenizabledoc.simple_tokenizer</span> <span class="kn">import</span> <span class="n">SimpleTokenizer</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
</pre></div>
</div>
<p>Setup a logger.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span><span class="p">,</span> <span class="n">StreamHandler</span><span class="p">,</span> <span class="n">NullHandler</span><span class="p">,</span> <span class="n">DEBUG</span><span class="p">,</span> <span class="n">ERROR</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;accelbrainbase&quot;</span><span class="p">)</span>
<span class="n">handler</span> <span class="o">=</span> <span class="n">StreamHandler</span><span class="p">()</span>
<span class="n">handler</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
</pre></div>
</div>
<p>Initialize a tokenizer and a vectorizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># `str` of your document.</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;Your document.&quot;</span>

<span class="n">nlp_base</span> <span class="o">=</span> <span class="n">NlpBase</span><span class="p">()</span>
<span class="n">nlp_base</span><span class="o">.</span><span class="n">delimiter_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
<span class="n">tokenizable_doc</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">()</span>
<span class="n">sentence_list</span> <span class="o">=</span> <span class="n">nlp_base</span><span class="o">.</span><span class="n">listup_sentence</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizable_doc</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
</pre></div>
</div>
<p>Setup the vectorizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizable_token</span> <span class="o">=</span> <span class="n">THotVectorizer</span><span class="p">(</span><span class="n">token_list</span><span class="o">=</span><span class="n">token_arr</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="n">vectorizable_token</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">token_list</span><span class="o">=</span><span class="n">token_arr</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">vector_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vector_list</span><span class="p">)</span>
<span class="n">token_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">token_list</span><span class="p">)</span>

<span class="n">token_iterator</span> <span class="o">=</span> <span class="n">TokenIterator</span><span class="p">(</span>
    <span class="n">vectorizable_token</span><span class="o">=</span><span class="n">vectorizable_token</span><span class="p">,</span> 
    <span class="n">token_arr</span><span class="o">=</span><span class="n">token_arr</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">norm_mode</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ctx</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">observed_arr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">token_iterator</span><span class="o">.</span><span class="n">generate_learned_samples</span><span class="p">():</span>
    <span class="k">break</span>
<span class="k">print</span><span class="p">(</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch size, the length of series, dimension)</span>
</pre></div>
</div>
<p>Instantiate <code class="docutils literal notranslate"><span class="pre">EncDecAD</span></code> and input hyperparameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstractable_semantics</span> <span class="o">=</span> <span class="n">EncDecAD</span><span class="p">(</span>
    <span class="c1"># The default parameter. The number of units in hidden layers.</span>
    <span class="n">hidden_neuron_count</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="c1"># The default parameter. The number of units in output layer.</span>
    <span class="n">output_neuron_count</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="c1"># The rate of dropout.</span>
    <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="c1"># Batch size.</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="c1"># Learning rate.</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
    <span class="c1"># The length of series.</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="c1"># `mx.gpu()` or `mx.cpu()`.</span>
    <span class="n">ctx</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Execute <code class="docutils literal notranslate"><span class="pre">learn</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstractable_semantics</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span>
    <span class="c1"># is-a `TokenIterator`.</span>
    <span class="n">token_iterator</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Execute <code class="docutils literal notranslate"><span class="pre">summarize</span></code> method to extract summaries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">abstract_list</span> <span class="o">=</span> <span class="n">abstractable_semantics</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span>
    <span class="c1"># is-a `TokenIterator`.</span>
    <span class="n">token_iterator</span><span class="p">,</span>
    <span class="c1"># is-a `VectorizableToken`.</span>
    <span class="n">vectorizable_token</span><span class="p">,</span>
    <span class="c1"># `list` of `str`, extracted by `nlp_base.listup_sentence(document)`.</span>
    <span class="n">sentence_list</span><span class="p">,</span>
    <span class="c1"># The number of extracted sentences.</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">abstract_list</span></code> is a <code class="docutils literal notranslate"><span class="pre">list</span></code> that contains <code class="docutils literal notranslate"><span class="pre">str</span></code>s of sentences.</p>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<span id="references"></span><h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Boulanger-Lewandowski, N., Bengio, Y., &amp; Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. arXiv preprint arXiv:1206.6392.</li>
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</li>
<li>Luhn, Hans Peter. “The automatic creation of literature abstracts.” IBM Journal of research and development 2.2 (1958): 159-165.</li>
<li>Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</li>
<li>Matthew A. Russell　著、佐藤 敏紀、瀬戸口 光宏、原川 浩一　監訳、長尾 高弘　訳『入門 ソーシャルデータ 第2版――ソーシャルウェブのデータマイニング』 2014年06月 発行</li>
<li>Sutskever, I., Hinton, G. E., &amp; Taylor, G. W. (2009). The recurrent temporal restricted boltzmann machine. In Advances in Neural Information Processing Systems (pp. 1601-1608).</li>
<li>Zhang, K., Grauman, K., &amp; Sha, F. (2018). Retrospective Encoders for Video Summarization. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 383-399).</li>
</ul>
<div class="section" id="related-poc">
<span id="related-poc"></span><h2>Related PoC<a class="headerlink" href="#related-poc" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://accel-brain.com/social-evolution-of-exploration-and-exposure-of-paradox-by-web-crawling-type-artificial-intelligence/">Webクローラ型人工知能によるパラドックス探索暴露機能の社会進化論</a> (Japanese)<ul>
<li><a class="reference external" href="https://accel-brain.com/social-evolution-of-exploration-and-exposure-of-paradox-by-web-crawling-type-artificial-intelligence/sozialstruktur-des-world-wide-web-und-semantik-der-kunstlichen-intelligenz-des-web-crawlers/">World-Wide Webの社会構造とWebクローラ型人工知能の意味論</a></li>
<li><a class="reference external" href="https://accel-brain.com/social-evolution-of-exploration-and-exposure-of-paradox-by-web-crawling-type-artificial-intelligence/semantik-der-semantik-und-beobachtung-der-beobachtung/">意味論の意味論、観察の観察</a></li>
</ul>
</li>
<li><a class="reference external" href="https://accel-brain.com/semantics-of-natural-language-processing-driven-by-bayesian-information-search-by-deep-reinforcement-learning/">深層強化学習のベイズ主義的な情報探索に駆動された自然言語処理の意味論</a> (Japanese)<ul>
<li><a class="reference external" href="https://accel-brain.com/semantics-of-natural-language-processing-driven-by-bayesian-information-search-by-deep-reinforcement-learning/tiefe-boltzmann-maschine-als-selbstkodierer/">平均場近似推論の統計力学、自己符号化器としての深層ボルツマンマシン</a></li>
<li><a class="reference external" href="https://accel-brain.com/semantics-of-natural-language-processing-driven-by-bayesian-information-search-by-deep-reinforcement-learning/regularisierungsproblem-und-gan/">正則化問題における敵対的生成ネットワーク(GANs)と敵対的自己符号化器(AAEs)のネットワーク構造</a></li>
<li><a class="reference external" href="https://accel-brain.com/semantics-of-natural-language-processing-driven-by-bayesian-information-search-by-deep-reinforcement-learning/naturliche-sprachverarbeitung-des-neuronalen-netzwerkmodells-und-der-netzwerkstruktur-eines-rekursiven-neuronalen-netzwerks/">ニューラルネットワーク言語モデルの自然言語処理と再帰的ニューラルネットワークのネットワーク構造</a></li>
<li><a class="reference external" href="https://accel-brain.com/semantics-of-natural-language-processing-driven-by-bayesian-information-search-by-deep-reinforcement-learning/naturliche-sprachverarbeitung-von-paradoxien-und-paradoxien-durch-naturliche-sprachverarbeitung/">自然言語処理のパラドックス、パラドックスの自然言語処理</a></li>
</ul>
</li>
<li><a class="reference external" href="https://accel-brain.com/data-modeling-von-korrespondenz-in-artificial-paradise/">「人工の理想」を背景とした「万物照応」のデータモデリング</a> (Japanese)<ul>
<li><a class="reference external" href="https://accel-brain.com/data-modeling-von-korrespondenz-in-artificial-paradise/web-crawler-als-funktionelles-aquivalent-des-flaneurs/">遊歩者の機能的等価物としてのWebクローラ、探索のアルゴリズムとアルゴリズムの探索</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="more-detail-demos">
<span id="more-detail-demos"></span><h2>More detail demos<a class="headerlink" href="#more-detail-demos" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://media.accel-brain.com/_chimera-network-is-web-crawling-ai/">Webクローラ型人工知能：キメラ・ネットワークの仕様</a> (Japanese)<ul>
<li>20001 bots are running as 20001 web-crawlers and 20001 web-scrapers.</li>
</ul>
</li>
<li><a class="reference external" href="https://media.accel-brain.com/category/agency-operation/">「代理演算」一覧 | Welcome to Singularity</a> (Japanese)<ul>
<li>20001 bots are running as 20001 blogers and 20001 “content curation writers”.</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="author">
<span id="author"></span><h2>Author<a class="headerlink" href="#author" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>accel-brain</li>
</ul>
</div>
<div class="section" id="author-uri">
<span id="author-uri"></span><h2>Author URI<a class="headerlink" href="#author-uri" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>https://accel-brain.co.jp/</li>
<li>https://accel-brain.com/</li>
</ul>
</div>
<div class="section" id="license">
<span id="license"></span><h2>License<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>GNU General Public License v2.0</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pysummarization.html" title="pysummarization package"
             >next</a> |</li>
        <li class="right" >
          <a href="index.html" title="Welcome to pysummarization’s documentation!"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pysummarization  documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Accel Brain.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>