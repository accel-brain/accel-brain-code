
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>pydbm.rnn package &#8212; pydbm  documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pydbm.rnn.interface package" href="pydbm.rnn.interface.html" />
    <link rel="prev" title="pydbm.optimization.optparams package" href="pydbm.optimization.optparams.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pydbm.rnn.interface.html" title="pydbm.rnn.interface package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="pydbm.optimization.optparams.html" title="pydbm.optimization.optparams package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pydbm  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="pydbm.html" accesskey="U">pydbm package</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="pydbm-rnn-package">
<h1>pydbm.rnn package<a class="headerlink" href="#pydbm-rnn-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="pydbm.rnn.interface.html">pydbm.rnn.interface package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.interface.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.interface.html#module-pydbm.rnn.interface.reconstructable_model">pydbm.rnn.interface.reconstructable_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.interface.html#module-pydbm.rnn.interface">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pydbm.rnn.lstmmodel.html">pydbm.rnn.lstmmodel package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.lstmmodel.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="pydbm.rnn.lstmmodel.convlstmmodel.html">pydbm.rnn.lstmmodel.convlstmmodel package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pydbm.rnn.lstmmodel.convlstmmodel.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydbm.rnn.lstmmodel.convlstmmodel.html#module-pydbm.rnn.lstmmodel.convlstmmodel.deconv_lstm_model">pydbm.rnn.lstmmodel.convlstmmodel.deconv_lstm_model module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydbm.rnn.lstmmodel.convlstmmodel.html#module-pydbm.rnn.lstmmodel.convlstmmodel">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.lstmmodel.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.lstmmodel.html#module-pydbm.rnn.lstmmodel.attention_lstm_model">pydbm.rnn.lstmmodel.attention_lstm_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.lstmmodel.html#module-pydbm.rnn.lstmmodel.conv_lstm_model">pydbm.rnn.lstmmodel.conv_lstm_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pydbm.rnn.lstmmodel.html#module-pydbm.rnn.lstmmodel">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pydbm.rnn.encoder_decoder_controller">
<span id="pydbm-rnn-encoder-decoder-controller-module"></span><h2>pydbm.rnn.encoder_decoder_controller module<a class="headerlink" href="#module-pydbm.rnn.encoder_decoder_controller" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController">
<em class="property">class </em><code class="descclassname">pydbm.rnn.encoder_decoder_controller.</code><code class="descname">EncoderDecoderController</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Encoder/Decoder based on LSTM networks.</p>
<p>This library provides Encoder/Decoder based on LSTM,
which is a reconstruction model and makes it possible to extract
series features embedded in deeper layers. The LSTM encoder learns
a fixed length vector of time-series observed data points and the
LSTM decoder uses this representation to reconstruct the time-series
using the current hidden state and the value inferenced at the previous time-step.</p>
<p>One interesting application example is the Encoder/Decoder for
Anomaly Detection (EncDec-AD) paradigm (Malhotra, P., et al. 2016).
This reconstruction model learns to reconstruct normal time-series behavior,
and thereafter uses reconstruction error to detect anomalies.
Malhotra, P., et al. (2016) showed that EncDec-AD paradigm is robust
and can detect anomalies from predictable, unpredictable, periodic, aperiodic,
and quasi-periodic time-series. Further, they showed that the paradigm is able
to detect anomalies from short time-series (length as small as 30) as well as
long time-series (length as large as 500).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_sine_wave_prediction_by_LSTM_encoder_decoder.ipynb">https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_sine_wave_prediction_by_LSTM_encoder_decoder.ipynb</a></li>
<li><a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_anomaly_detection_by_enc_dec_ad.ipynb">https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_anomaly_detection_by_enc_dec_ad.ipynb</a></li>
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</li>
<li>Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</li>
</ul>
<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.back_propagation">
<code class="descname">back_propagation</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.back_propagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>delta_output_arr</strong> – Delta.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple data.
- decoder’s <cite>list</cite> of gradations,
- encoder’s <cite>np.ndarray</cite> of Delta,
- encoder’s <cite>list</cite> of gradations.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.computable_loss">
<code class="descname">computable_loss</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.computable_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.decoder">
<code class="descname">decoder</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.encoder">
<code class="descname">encoder</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_computable_loss">
<code class="descname">get_computable_loss</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_computable_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_decoder">
<code class="descname">get_decoder</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_encoder">
<code class="descname">get_encoder</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_feature_points">
<code class="descname">get_feature_points</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_feature_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the activities in hidden layer and reset it,
considering this method will be called per one cycle in instances of time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of feature points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_reconstruction_error">
<code class="descname">get_reconstruction_error</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_reconstruction_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the reconstructed error in inferencing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of reconstruction error.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_verificatable_result">
<code class="descname">get_verificatable_result</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.get_verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.inference">
<code class="descname">inference</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference the feature points to reconstruct the time-series.</p>
<p>Override.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>hidden_activity_arr</strong> – Array like or sparse matrix as the state in hidden layer.</li>
<li><strong>cec_activity_arr</strong> – Array like or sparse matrix as the state in RNN.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- Array like or sparse matrix of reconstructed instances of time-series,
- Array like or sparse matrix of the state in hidden layer,
- Array like or sparse matrix of the state in RNN.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.learn">
<code class="descname">learn</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the observed data points
for vector representation of the input time-series.</p>
<p>Override.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>target_arr</strong> – Array like or sparse matrix as the target data points.
To learn as Auto-encoder, this value must be <cite>None</cite> or equivalent to <cite>observed_arr</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.learn_generated">
<code class="descname">learn_generated</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.learn_generated" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn features generated by <cite>FeatureGenerator</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>feature_generator</strong> – is-a <cite>FeatureGenerator</cite>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.load_pre_learned_params">
<code class="descname">load_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.load_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Load pre-learned parameters.</p>
<p>If you want to load pre-learned parameters simultaneously with stacked graphs,
call method <cite>stack_graph</cite> and setup the graphs before calling this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dir_path</strong> – Dir path.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.optimize">
<code class="descname">optimize</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.optimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>decoder_grads_list</strong> – decoder’s <cite>list</cite> of graduations.</li>
<li><strong>encoder_grads_list</strong> – encoder’s <cite>list</cite> of graduations.</li>
<li><strong>learning_rate</strong> – Learning rate.</li>
<li><strong>epoch</strong> – Now epoch.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.save_pre_learned_params">
<code class="descname">save_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.save_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Save pre-learned parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dir_path</strong> – Path of dir. If <cite>None</cite>, the file is saved in the current directory.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_computable_loss">
<code class="descname">set_computable_loss</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_computable_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_readonly">
<code class="descname">set_readonly</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_readonly" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_verificatable_result">
<code class="descname">set_verificatable_result</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.set_verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.verificatable_result">
<code class="descname">verificatable_result</code><a class="headerlink" href="#pydbm.rnn.encoder_decoder_controller.EncoderDecoderController.verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pydbm.rnn.facade_attention_encoder_decoder">
<span id="pydbm-rnn-facade-attention-encoder-decoder-module"></span><h2>pydbm.rnn.facade_attention_encoder_decoder module<a class="headerlink" href="#module-pydbm.rnn.facade_attention_encoder_decoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder">
<em class="property">class </em><code class="descclassname">pydbm.rnn.facade_attention_encoder_decoder.</code><code class="descname">FacadeAttentionEncoderDecoder</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p><cite>Facade</cite> for casual user of Encoder/Decoder based on LSTM networks with an Attention mechanism.</p>
<p>This library provides Encoder/Decoder based on LSTM with an Attention mechanism,
which is a reconstruction model and makes it possible to extract
series features embedded in deeper layers. The LSTM encoder learns
a fixed length vector of time-series observed data points and the
LSTM decoder uses this representation to reconstruct the time-series
using the current hidden state and the value inferenced at the previous time-step.</p>
<p>One interesting application example is the Encoder/Decoder for
Anomaly Detection (EncDec-AD) paradigm (Malhotra, P., et al. 2016).
This reconstruction model learns to reconstruct normal time-series behavior,
and thereafter uses reconstruction error to detect anomalies.
Malhotra, P., et al. (2016) showed that EncDec-AD paradigm is robust
and can detect anomalies from predictable, unpredictable, periodic, aperiodic,
and quasi-periodic time-series. Further, they showed that the paradigm is able
to detect anomalies from short time-series (length as small as 30) as well as
long time-series (length as large as 500).</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</li>
</ul>
<dl class="attribute">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.get_feature_points">
<code class="descname">get_feature_points</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.get_feature_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the activities in hidden layer and reset it,
considering this method will be called per one cycle in instances of time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of feature points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.get_reconstruction_error">
<code class="descname">get_reconstruction_error</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.get_reconstruction_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the reconstructed error in inferencing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of reconstruction error.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.infernece">
<code class="descname">infernece</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.infernece" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference the feature points to reconstruct the time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>hidden_activity_arr</strong> – Array like or sparse matrix as the state in hidden layer.</li>
<li><strong>cec_activity_arr</strong> – Array like or sparse matrix as the state in RNN.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- Array like or sparse matrix of reconstructed instances of time-series,
- Array like or sparse matrix of the state in hidden layer,
- Array like or sparse matrix of the state in RNN.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.learn">
<code class="descname">learn</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the observed data points
for vector representation of the input time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>target_arr</strong> – Array like or sparse matrix as the target data points.
To learn as Auto-encoder, this value must be <cite>None</cite> or equivalent to <cite>observed_arr</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.save_pre_learned_params">
<code class="descname">save_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.facade_attention_encoder_decoder.FacadeAttentionEncoderDecoder.save_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Save pre-learned parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>encoder_file_path</strong> – File path.</li>
<li><strong>decoder_file_path</strong> – File path.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pydbm.rnn.facade_encoder_decoder">
<span id="pydbm-rnn-facade-encoder-decoder-module"></span><h2>pydbm.rnn.facade_encoder_decoder module<a class="headerlink" href="#module-pydbm.rnn.facade_encoder_decoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder">
<em class="property">class </em><code class="descclassname">pydbm.rnn.facade_encoder_decoder.</code><code class="descname">FacadeEncoderDecoder</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p><cite>Facade</cite> for casual user of Encoder/Decoder based on LSTM networks.</p>
<p>This library provides Encoder/Decoder based on LSTM,
which is a reconstruction model and makes it possible to extract
series features embedded in deeper layers. The LSTM encoder learns
a fixed length vector of time-series observed data points and the
LSTM decoder uses this representation to reconstruct the time-series
using the current hidden state and the value inferenced at the previous time-step.</p>
<p>One interesting application example is the Encoder/Decoder for
Anomaly Detection (EncDec-AD) paradigm (Malhotra, P., et al. 2016).
This reconstruction model learns to reconstruct normal time-series behavior,
and thereafter uses reconstruction error to detect anomalies.
Malhotra, P., et al. (2016) showed that EncDec-AD paradigm is robust
and can detect anomalies from predictable, unpredictable, periodic, aperiodic,
and quasi-periodic time-series. Further, they showed that the paradigm is able
to detect anomalies from short time-series (length as small as 30) as well as
long time-series (length as large as 500).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_sine_wave_prediction_by_LSTM_encoder_decoder.ipynb">https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_sine_wave_prediction_by_LSTM_encoder_decoder.ipynb</a></li>
<li><a class="reference external" href="https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_anomaly_detection_by_enc_dec_ad.ipynb">https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_anomaly_detection_by_enc_dec_ad.ipynb</a></li>
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</li>
<li>Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</li>
</ul>
<dl class="attribute">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.get_feature_points">
<code class="descname">get_feature_points</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.get_feature_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the activities in hidden layer and reset it,
considering this method will be called per one cycle in instances of time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of feature points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.get_reconstruction_error">
<code class="descname">get_reconstruction_error</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.get_reconstruction_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the reconstructed error in inferencing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The array like or sparse matrix of reconstruction error.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.infernece">
<code class="descname">infernece</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.infernece" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference the feature points to reconstruct the time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>hidden_activity_arr</strong> – Array like or sparse matrix as the state in hidden layer.</li>
<li><strong>cec_activity_arr</strong> – Array like or sparse matrix as the state in RNN.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- Array like or sparse matrix of reconstructed instances of time-series,
- Array like or sparse matrix of the state in hidden layer,
- Array like or sparse matrix of the state in RNN.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.learn">
<code class="descname">learn</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the observed data points
for vector representation of the input time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data ponts.</li>
<li><strong>target_arr</strong> – Array like or sparse matrix as the target data points.
To learn as Auto-encoder, this value must be <cite>None</cite> or equivalent to <cite>observed_arr</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.save_pre_learned_params">
<code class="descname">save_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.facade_encoder_decoder.FacadeEncoderDecoder.save_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Save pre-learned parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>encoder_file_path</strong> – File path.</li>
<li><strong>decoder_file_path</strong> – File path.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pydbm.rnn.lstm_model">
<span id="pydbm-rnn-lstm-model-module"></span><h2>pydbm.rnn.lstm_model module<a class="headerlink" href="#module-pydbm.rnn.lstm_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydbm.rnn.lstm_model.LSTMModel">
<em class="property">class </em><code class="descclassname">pydbm.rnn.lstm_model.</code><code class="descname">LSTMModel</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pydbm.rnn.interface.html#pydbm.rnn.interface.reconstructable_model.ReconstructableModel" title="pydbm.rnn.interface.reconstructable_model.ReconstructableModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pydbm.rnn.interface.reconstructable_model.ReconstructableModel</span></code></a></p>
<p>Long short term memory(LSTM) networks.</p>
<p>Originally, Long Short-Term Memory(LSTM) networks as a
special RNN structure has proven stable and powerful for
modeling long-range dependencies.</p>
<p>The Key point of structural expansion is its memory cell
which essentially acts as an accumulator of the state information.
Every time observed data points are given as new information and
input to LSTM’s input gate, its information will be accumulated to
the cell if the input gate is activated. The past state of cell
could be forgotten in this process if LSTM’s forget gate is on.
Whether the latest cell output will be propagated to the final state
is further controlled by the output gate.</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</li>
<li>Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</li>
<li>Zaremba, W., Sutskever, I., &amp; Vinyals, O. (2014). Recurrent neural network regularization. arXiv preprint arXiv:1409.2329.</li>
</ul>
<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.back_propagation">
<code class="descname">back_propagation</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.back_propagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>pred_arr</strong> – <cite>np.ndarray</cite> of predicted data points.</li>
<li><strong>delta_output_arr</strong> – Delta.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- <cite>np.ndarray</cite> of Delta,
- <cite>list</cite> of gradations</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.forward_propagation">
<code class="descname">forward_propagation</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.forward_propagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>batch_observed_arr</strong> – Array like or sparse matrix as the observed data points.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Array like or sparse matrix as the predicted data points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.get_feature_points">
<code class="descname">get_feature_points</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.get_feature_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the activities in hidden layer and reset it,
considering this method will be called per one cycle in instances of time-series.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">The <cite>list</cite> of array like or sparse matrix of feature points or virtual visible observed data points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.get_graph">
<code class="descname">get_graph</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.get_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.get_opt_params">
<code class="descname">get_opt_params</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.get_opt_params" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.get_verificatable_result">
<code class="descname">get_verificatable_result</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.get_verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.get_weight_decay_term">
<code class="descname">get_weight_decay_term</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.get_weight_decay_term" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.graph">
<code class="descname">graph</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.hidden_back_propagate">
<code class="descname">hidden_back_propagate</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.hidden_back_propagate" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation in hidden layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>delta_output_arr</strong> – Delta.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple data.
- <cite>np.ndarray</cite> of Delta in observed data points,
- <cite>np.ndarray</cite> of Delta in hidden units,
- <cite>list</cite> of gradations.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.hidden_forward_propagate">
<code class="descname">hidden_forward_propagate</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.hidden_forward_propagate" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation in LSTM gate.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>observed_arr</strong> – <cite>np.ndarray</cite> of observed data points.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Predicted data points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.inference">
<code class="descname">inference</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference the feature points to reconstruct the time-series.</p>
<p>Override.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data points.</li>
<li><strong>hidden_activity_arr</strong> – Array like or sparse matrix as the state in hidden layer.</li>
<li><strong>cec_activity_arr</strong> – Array like or sparse matrix as the state in the constant error carousel.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- Array like or sparse matrix of reconstructed instances of time-series,
- Array like or sparse matrix of the state in hidden layer,
- Array like or sparse matrix of the state in RNN.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.learn">
<code class="descname">learn</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the observed data points
for vector representation of the input time-series.</p>
<p>Override.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>observed_arr</strong> – Array like or sparse matrix as the observed data points.
The shape is: (batch size, the length of sequences, feature points)</li>
<li><strong>target_arr</strong> – Array like or sparse matrix as the target data points.
To learn as Auto-encoder, this value must be <cite>None</cite> or equivalent to <cite>observed_arr</cite>.
The shape is: (batch size, labeled data)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.load_pre_learned_params">
<code class="descname">load_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.load_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Load pre-learned parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dir_name</strong> – Path of dir. If <cite>None</cite>, the file is saved in the current directory.</li>
<li><strong>file_name</strong> – File name.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.lstm_backward">
<code class="descname">lstm_backward</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.lstm_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation in LSTM gate.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>delta_hidden_arr</strong> – Delta from output layer to hidden layer.</li>
<li><strong>delta_cec_arr</strong> – Delta in LSTM gate.</li>
<li><strong>cycle</strong> – Now cycle or time.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- Delta from hidden layer to input layer,
- Delta in hidden layer at previous time,
- Delta in LSTM gate at previous time,
- <cite>list</cite> of gradations.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.opt_params">
<code class="descname">opt_params</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.opt_params" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.optimize">
<code class="descname">optimize</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.optimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>grads_list</strong> – <cite>list</cite> of graduations.</li>
<li><strong>learning_rate</strong> – Learning rate.</li>
<li><strong>epoch</strong> – Now epoch.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.output_back_propagate">
<code class="descname">output_back_propagate</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.output_back_propagate" title="Permalink to this definition">¶</a></dt>
<dd><p>Back propagation in output layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>pred_arr</strong> – <cite>np.ndarray</cite> of predicted data points.</li>
<li><strong>delta_output_arr</strong> – Delta.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple data.
- <cite>np.ndarray</cite> of Delta,
- <cite>list</cite> of gradations.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.output_forward_propagate">
<code class="descname">output_forward_propagate</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.output_forward_propagate" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation in output layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pred_arr</strong> – <cite>np.ndarray</cite> of predicted data points.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><cite>np.ndarray</cite> of propagated data points.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.save_pre_learned_params">
<code class="descname">save_pre_learned_params</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.save_pre_learned_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Save pre-learned parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dir_name</strong> – Path of dir. If <cite>None</cite>, the file is saved in the current directory.</li>
<li><strong>file_name</strong> – File name.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.set_graph">
<code class="descname">set_graph</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.set_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.set_opt_params">
<code class="descname">set_opt_params</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.set_opt_params" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.set_verificatable_result">
<code class="descname">set_verificatable_result</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.set_verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.set_weight_decay_term">
<code class="descname">set_weight_decay_term</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.set_weight_decay_term" title="Permalink to this definition">¶</a></dt>
<dd><p>setter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.verificatable_result">
<code class="descname">verificatable_result</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.verificatable_result" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

<dl class="attribute">
<dt id="pydbm.rnn.lstm_model.LSTMModel.weight_decay_term">
<code class="descname">weight_decay_term</code><a class="headerlink" href="#pydbm.rnn.lstm_model.LSTMModel.weight_decay_term" title="Permalink to this definition">¶</a></dt>
<dd><p>getter</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pydbm.rnn">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pydbm.rnn" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="pydbm.optimization.optparams.html"
                        title="previous chapter">pydbm.optimization.optparams package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pydbm.rnn.interface.html"
                        title="next chapter">pydbm.rnn.interface package</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pydbm.rnn.interface.html" title="pydbm.rnn.interface package"
             >next</a> |</li>
        <li class="right" >
          <a href="pydbm.optimization.optparams.html" title="pydbm.optimization.optparams package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pydbm  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="pydbm.html" >pydbm package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018-2019, chimera0(RUM).
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>