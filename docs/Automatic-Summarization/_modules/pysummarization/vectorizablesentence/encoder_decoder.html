

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>pysummarization.vectorizablesentence.encoder_decoder &#8212; pysummarization  documentation</title>
    <link rel="stylesheet" href="../../../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">pysummarization  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pysummarization.vectorizablesentence.encoder_decoder</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="k">import</span> <span class="n">getLogger</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pysummarization.vectorizable_sentence</span> <span class="k">import</span> <span class="n">VectorizableSentence</span>

<span class="c1"># LSTM Graph which is-a `Synapse`.</span>
<span class="kn">from</span> <span class="nn">pydbm.synapse.recurrenttemporalgraph.lstm_graph</span> <span class="k">import</span> <span class="n">LSTMGraph</span>
<span class="kn">from</span> <span class="nn">pydbm.synapse.recurrenttemporalgraph.lstm_graph</span> <span class="k">import</span> <span class="n">LSTMGraph</span> <span class="k">as</span> <span class="n">EncoderGraph</span>
<span class="kn">from</span> <span class="nn">pydbm.synapse.recurrenttemporalgraph.lstm_graph</span> <span class="k">import</span> <span class="n">LSTMGraph</span> <span class="k">as</span> <span class="n">DecoderGraph</span>

<span class="c1"># Loss function.</span>
<span class="kn">from</span> <span class="nn">pydbm.loss.mean_squared_error</span> <span class="k">import</span> <span class="n">MeanSquaredError</span>
<span class="c1"># Adam as a Loss function.</span>
<span class="kn">from</span> <span class="nn">pydbm.optimization.optparams.adam</span> <span class="k">import</span> <span class="n">Adam</span> <span class="k">as</span> <span class="n">EncoderAdam</span>
<span class="kn">from</span> <span class="nn">pydbm.optimization.optparams.adam</span> <span class="k">import</span> <span class="n">Adam</span> <span class="k">as</span> <span class="n">DecoderAdam</span>
<span class="c1"># Verification.</span>
<span class="kn">from</span> <span class="nn">pydbm.verification.verificate_function_approximation</span> <span class="k">import</span> <span class="n">VerificateFunctionApproximation</span>
<span class="c1"># LSTM model.</span>
<span class="kn">from</span> <span class="nn">pydbm.rnn.lstm_model</span> <span class="k">import</span> <span class="n">LSTMModel</span> <span class="k">as</span> <span class="n">Encoder</span>
<span class="kn">from</span> <span class="nn">pydbm.rnn.lstm_model</span> <span class="k">import</span> <span class="n">LSTMModel</span> <span class="k">as</span> <span class="n">Decoder</span>
<span class="c1"># Logistic Function as activation function.</span>
<span class="kn">from</span> <span class="nn">pydbm.activation.logistic_function</span> <span class="k">import</span> <span class="n">LogisticFunction</span>
<span class="c1"># Tanh Function as activation function.</span>
<span class="kn">from</span> <span class="nn">pydbm.activation.tanh_function</span> <span class="k">import</span> <span class="n">TanhFunction</span>
<span class="c1"># Encoder/Decoder</span>
<span class="kn">from</span> <span class="nn">pydbm.rnn.encoder_decoder_controller</span> <span class="k">import</span> <span class="n">EncoderDecoderController</span>


<div class="viewcode-block" id="EncoderDecoder"><a class="viewcode-back" href="../../../pysummarization.vectorizablesentence.html#pysummarization.vectorizablesentence.encoder_decoder.EncoderDecoder">[docs]</a><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">VectorizableSentence</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Vectorize sentences by Encoder/Decoder based on LSTM.</span>

<span class="sd">    This library provides Encoder/Decoder based on LSTM, </span>
<span class="sd">    which is a reconstruction model and makes it possible to </span>
<span class="sd">    extract series features embedded in deeper layers. </span>
<span class="sd">    The LSTM encoder learns a fixed length vector of time-series </span>
<span class="sd">    observed data points and the LSTM decoder uses this representation </span>
<span class="sd">    to reconstruct the time-series using the current hidden state </span>
<span class="sd">    and the value inferenced at the previous time-step.</span>

<span class="sd">    References:</span>
<span class="sd">        - https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_sine_wave_prediction_by_LSTM_encoder_decoder.ipynb</span>
<span class="sd">        - https://github.com/chimera0/accel-brain-code/blob/master/Deep-Learning-by-means-of-Design-Pattern/demo/demo_anomaly_detection_by_enc_dec_ad.ipynb</span>
<span class="sd">        - Cho, K., Van MerriÃ«nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</span>
<span class="sd">        - Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</span>

<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; Init. &#39;&#39;&#39;</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pysummarization&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__logger</span> <span class="o">=</span> <span class="n">logger</span>

<div class="viewcode-block" id="EncoderDecoder.vectorize"><a class="viewcode-back" href="../../../pysummarization.vectorizablesentence.html#pysummarization.vectorizablesentence.encoder_decoder.EncoderDecoder.vectorize">[docs]</a>    <span class="k">def</span> <span class="nf">vectorize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence_list</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Args:</span>
<span class="sd">            sentence_list:   The list of tokenized sentences.</span>
<span class="sd">                             [[`token`, `token`, `token`, ...],</span>
<span class="sd">                             [`token`, `token`, `token`, ...],</span>
<span class="sd">                             [`token`, `token`, `token`, ...]]</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            `np.ndarray` of tokens.</span>
<span class="sd">            [vector of token, vector of token, vector of token]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">test_observed_arr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__setup_dataset</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__token_master_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__sentence_mean_len</span><span class="p">)</span>
        <span class="n">pred_arr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__controller</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">test_observed_arr</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__controller</span><span class="o">.</span><span class="n">get_feature_points</span><span class="p">()</span></div>

<div class="viewcode-block" id="EncoderDecoder.learn"><a class="viewcode-back" href="../../../pysummarization.vectorizablesentence.html#pysummarization.vectorizablesentence.encoder_decoder.EncoderDecoder.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentence_list</span><span class="p">,</span>
        <span class="n">token_master_list</span><span class="p">,</span>        
        <span class="n">hidden_neuron_count</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
        <span class="n">learning_attenuate_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">attenuate_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">weight_limit</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">test_size_rate</span><span class="o">=</span><span class="mf">0.3</span>
    <span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Init.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            sentence_list:                  The list of tokenized sentences.</span>
<span class="sd">                                            [[`token`, `token`, `token`, ...],</span>
<span class="sd">                                            [`token`, `token`, `token`, ...],</span>
<span class="sd">                                            [`token`, `token`, `token`, ...]]</span>

<span class="sd">            token_master_list:              Unique `list` of tokens.</span>
<span class="sd">            hidden_neuron_count:            The number of units in hidden layer.</span>
<span class="sd">            epochs:                         Epochs of Mini-batch.</span>
<span class="sd">            batch_size:                     Batch size of Mini-batch.</span>
<span class="sd">            learning_rate:                  Learning rate.</span>
<span class="sd">            learning_attenuate_rate:        Attenuate the `learning_rate` by a factor of this value every `attenuate_epoch`.</span>
<span class="sd">            attenuate_epoch:                Attenuate the `learning_rate` by a factor of `learning_attenuate_rate` every `attenuate_epoch`.</span>
<span class="sd">                                            Additionally, in relation to regularization,</span>
<span class="sd">                                            this class constrains weight matrixes every `attenuate_epoch`.</span>

<span class="sd">            weight_limit:                   Regularization for weights matrix</span>
<span class="sd">                                            to repeat multiplying the weights matrix and `0.9`</span>
<span class="sd">                                            until $\sum_{j=0}^{n}w_{ji}^2 &lt; weight\_limit$.</span>

<span class="sd">            dropout_rate:                   The probability of dropout.</span>
<span class="sd">            test_size_rate:                 Size of Test data set. If this value is `0`, the </span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">observed_arr</span><span class="p">,</span> <span class="n">sentence_mean_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__setup_dataset</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">,</span> <span class="n">token_master_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__sentence_mean_len</span> <span class="o">=</span> <span class="n">sentence_mean_len</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">__logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Shape of observed data points:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># Init.</span>
        <span class="n">encoder_graph</span> <span class="o">=</span> <span class="n">EncoderGraph</span><span class="p">()</span>

        <span class="c1"># Activation function in LSTM.</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">observed_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">input_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">forget_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">output_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">hidden_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">output_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>

        <span class="c1"># Initialization strategy.</span>
        <span class="c1"># This method initialize each weight matrices and biases in Gaussian distribution: `np.random.normal(size=hoge) * 0.01`.</span>
        <span class="n">encoder_graph</span><span class="o">.</span><span class="n">create_rnn_cells</span><span class="p">(</span>
            <span class="n">input_neuron_count</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">hidden_neuron_count</span><span class="o">=</span><span class="n">hidden_neuron_count</span><span class="p">,</span>
            <span class="n">output_neuron_count</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Init.</span>
        <span class="n">decoder_graph</span> <span class="o">=</span> <span class="n">DecoderGraph</span><span class="p">()</span>

        <span class="c1"># Activation function in LSTM.</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">observed_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">input_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">forget_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">output_gate_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">hidden_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">output_activating_function</span> <span class="o">=</span> <span class="n">LogisticFunction</span><span class="p">()</span>

        <span class="c1"># Initialization strategy.</span>
        <span class="c1"># This method initialize each weight matrices and biases in Gaussian distribution: `np.random.normal(size=hoge) * 0.01`.</span>
        <span class="n">decoder_graph</span><span class="o">.</span><span class="n">create_rnn_cells</span><span class="p">(</span>
            <span class="n">input_neuron_count</span><span class="o">=</span><span class="n">hidden_neuron_count</span><span class="p">,</span>
            <span class="n">hidden_neuron_count</span><span class="o">=</span><span class="n">hidden_neuron_count</span><span class="p">,</span>
            <span class="n">output_neuron_count</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">encoder_opt_params</span> <span class="o">=</span> <span class="n">EncoderAdam</span><span class="p">()</span>
        <span class="n">encoder_opt_params</span><span class="o">.</span><span class="n">weight_limit</span> <span class="o">=</span> <span class="n">weight_limit</span>
        <span class="n">encoder_opt_params</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>

        <span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span>
            <span class="c1"># Delegate `graph` to `LSTMModel`.</span>
            <span class="n">graph</span><span class="o">=</span><span class="n">encoder_graph</span><span class="p">,</span>
            <span class="c1"># The number of epochs in mini-batch training.</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="c1"># The batch size.</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="c1"># Learning rate.</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="c1"># Attenuate the `learning_rate` by a factor of this value every `attenuate_epoch`.</span>
            <span class="n">learning_attenuate_rate</span><span class="o">=</span><span class="n">learning_attenuate_rate</span><span class="p">,</span>
            <span class="c1"># Attenuate the `learning_rate` by a factor of `learning_attenuate_rate` every `attenuate_epoch`.</span>
            <span class="n">attenuate_epoch</span><span class="o">=</span><span class="n">attenuate_epoch</span><span class="p">,</span>
            <span class="c1"># Refereed maxinum step `t` in BPTT. If `0`, this class referes all past data in BPTT.</span>
            <span class="n">bptt_tau</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="c1"># Size of Test data set. If this value is `0`, the validation will not be executed.</span>
            <span class="n">test_size_rate</span><span class="o">=</span><span class="n">test_size_rate</span><span class="p">,</span>
            <span class="c1"># Loss function.</span>
            <span class="n">computable_loss</span><span class="o">=</span><span class="n">MeanSquaredError</span><span class="p">(),</span>
            <span class="c1"># Optimizer.</span>
            <span class="n">opt_params</span><span class="o">=</span><span class="n">encoder_opt_params</span><span class="p">,</span>
            <span class="c1"># Verification function.</span>
            <span class="n">verificatable_result</span><span class="o">=</span><span class="n">VerificateFunctionApproximation</span><span class="p">(),</span>
            <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span>
        <span class="p">)</span>

        <span class="n">decoder_opt_params</span> <span class="o">=</span> <span class="n">DecoderAdam</span><span class="p">()</span>
        <span class="n">decoder_opt_params</span><span class="o">.</span><span class="n">weight_limit</span> <span class="o">=</span> <span class="n">weight_limit</span>
        <span class="n">decoder_opt_params</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>

        <span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span>
            <span class="c1"># Delegate `graph` to `LSTMModel`.</span>
            <span class="n">graph</span><span class="o">=</span><span class="n">decoder_graph</span><span class="p">,</span>
            <span class="c1"># The number of epochs in mini-batch training.</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="c1"># The batch size.</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="c1"># Learning rate.</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="c1"># Attenuate the `learning_rate` by a factor of this value every `attenuate_epoch`.</span>
            <span class="n">learning_attenuate_rate</span><span class="o">=</span><span class="n">learning_attenuate_rate</span><span class="p">,</span>
            <span class="c1"># Attenuate the `learning_rate` by a factor of `learning_attenuate_rate` every `attenuate_epoch`.</span>
            <span class="n">attenuate_epoch</span><span class="o">=</span><span class="n">attenuate_epoch</span><span class="p">,</span>
            <span class="c1"># The length of sequences.</span>
            <span class="n">seq_len</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="c1"># Refereed maxinum step `t` in BPTT. If `0`, this class referes all past data in BPTT.</span>
            <span class="n">bptt_tau</span><span class="o">=</span><span class="n">observed_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="c1"># Size of Test data set. If this value is `0`, the validation will not be executed.</span>
            <span class="n">test_size_rate</span><span class="o">=</span><span class="n">test_size_rate</span><span class="p">,</span>
            <span class="c1"># Loss function.</span>
            <span class="n">computable_loss</span><span class="o">=</span><span class="n">MeanSquaredError</span><span class="p">(),</span>
            <span class="c1"># Optimizer.</span>
            <span class="n">opt_params</span><span class="o">=</span><span class="n">decoder_opt_params</span><span class="p">,</span>
            <span class="c1"># Verification function.</span>
            <span class="n">verificatable_result</span><span class="o">=</span><span class="n">VerificateFunctionApproximation</span><span class="p">(),</span>
            <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span>
        <span class="p">)</span>

        <span class="n">encoder_decoder_controller</span> <span class="o">=</span> <span class="n">EncoderDecoderController</span><span class="p">(</span>
            <span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
            <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">learning_attenuate_rate</span><span class="o">=</span><span class="n">learning_attenuate_rate</span><span class="p">,</span>
            <span class="n">attenuate_epoch</span><span class="o">=</span><span class="n">attenuate_epoch</span><span class="p">,</span>
            <span class="n">test_size_rate</span><span class="o">=</span><span class="n">test_size_rate</span><span class="p">,</span>
            <span class="n">computable_loss</span><span class="o">=</span><span class="n">MeanSquaredError</span><span class="p">(),</span>
            <span class="n">verificatable_result</span><span class="o">=</span><span class="n">VerificateFunctionApproximation</span><span class="p">(),</span>
            <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span>
        <span class="p">)</span>

        <span class="c1"># Learning.</span>
        <span class="n">encoder_decoder_controller</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">observed_arr</span><span class="p">,</span> <span class="n">observed_arr</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">__controller</span> <span class="o">=</span> <span class="n">encoder_decoder_controller</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__token_master_list</span> <span class="o">=</span> <span class="n">token_master_list</span></div>

    <span class="k">def</span> <span class="nf">__setup_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence_list</span><span class="p">,</span> <span class="n">token_master_list</span><span class="p">,</span> <span class="n">sentence_mean_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sentence_mean_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sentence_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">)):</span>
                <span class="n">sentence_len_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">sentence_mean_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">sentence_len_list</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence_len_list</span><span class="p">))</span>

        <span class="n">observed_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence_list</span><span class="p">)):</span>
            <span class="n">arr_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">sentence_mean_len</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sentence_mean_len</span><span class="p">):</span>
                <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_master_list</span><span class="p">))</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">token</span> <span class="o">=</span> <span class="n">sentence_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">arr</span><span class="p">[</span><span class="n">token_master_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">token</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                    <span class="k">pass</span>
                <span class="k">finally</span><span class="p">:</span>
                    <span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                    <span class="n">arr_list</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span>
            <span class="n">observed_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr_list</span>
        <span class="n">observed_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">observed_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">observed_arr</span><span class="p">,</span> <span class="n">sentence_mean_len</span>

<div class="viewcode-block" id="EncoderDecoder.get_controller"><a class="viewcode-back" href="../../../pysummarization.vectorizablesentence.html#pysummarization.vectorizablesentence.encoder_decoder.EncoderDecoder.get_controller">[docs]</a>    <span class="k">def</span> <span class="nf">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; getter &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__controller</span></div>

<div class="viewcode-block" id="EncoderDecoder.set_readonly"><a class="viewcode-back" href="../../../pysummarization.vectorizablesentence.html#pysummarization.vectorizablesentence.encoder_decoder.EncoderDecoder.set_readonly">[docs]</a>    <span class="k">def</span> <span class="nf">set_readonly</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; setter &#39;&#39;&#39;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;This property must be read-only.&quot;</span><span class="p">)</span></div>
    
    <span class="n">controller</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="n">get_controller</span><span class="p">,</span> <span class="n">set_readonly</span><span class="p">)</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">pysummarization  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Accel Brain.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>