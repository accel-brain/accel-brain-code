

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>pygan package &#8212; pygan  documentation</title>
    <link rel="stylesheet" href="_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pygan" href="modules.html" />
    <link rel="prev" title="Generative Adversarial Networks Library: pygan" href="README.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="modules.html" title="pygan"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="README.html" title="Generative Adversarial Networks Library: pygan"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pygan  documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="README.html"
                        title="previous chapter">Generative Adversarial Networks Library: pygan</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="modules.html"
                        title="next chapter">pygan</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="pygan-package">
<h1>pygan package<a class="headerlink" href="#pygan-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pygan.ebaae_image_generator">
<span id="pygan-ebaae-image-generator-module"></span><h2>pygan.ebaae_image_generator module<a class="headerlink" href="#module-pygan.ebaae_image_generator" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pygan.ebaae_image_generator.EBAAEImageGenerator">
<em class="property">class </em><code class="descclassname">pygan.ebaae_image_generator.</code><code class="descname">EBAAEImageGenerator</code><span class="sig-paren">(</span><em>dir_list</em>, <em>width=28</em>, <em>height=28</em>, <em>channel=1</em>, <em>normal_height=14</em>, <em>normal_width=14</em>, <em>normal_channel=32</em>, <em>initializer=None</em>, <em>batch_size=40</em>, <em>learning_rate=0.001</em>, <em>ctx=gpu(0)</em>, <em>discriminative_model=None</em>, <em>generative_model=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/ebaae_image_generator.html#EBAAEImageGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.ebaae_image_generator.EBAAEImageGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Image generation by EBAAE.</p>
<p>The Generative Adversarial Networks(GANs) (Goodfellow et al., 2014) framework establishes
a min-max adversarial game between two neural networks – a generative model, <cite>G</cite>, and a
discriminative model, <cite>D</cite>. The discriminator model, <cite>D(x)</cite>, is a neural network that computes
the probability that a observed data point <cite>x</cite> in data space is a sample from the data
distribution (positive samples) that we are trying to model, rather than a sample from our
generative model (negative samples).</p>
<p>Concurrently, the generator uses a function <cite>G(z)</cite> that maps samples <cite>z</cite> from the prior <cite>p(z)</cite> to
the data space. <cite>G(z)</cite> is trained to maximally confuse the discriminator into believing that
samples it generates come from the data distribution. The generator is trained by leveraging
the gradient of <cite>D(x)</cite> w.r.t. x, and using that to modify its parameters.</p>
<p>The Conditional GANs (or cGANs) is a simple extension of the basic GAN model which allows
the model to condition on external information. This makes it possible to engage the learned
generative model in different “modes” by providing it with different contextual
information (Gauthier, J. 2014).</p>
<p>This model can be constructed by simply feeding the data, <cite>y</cite>, to condition on to both the generator
and discriminator. In an unconditioned generative model, because the maps samples <cite>z</cite> from the prior
<cite>p(z)</cite> are drawn from uniform or normal distribution, there is no control on modes of the data being
generated. On the other hand, it is possible to direct the data generation process by conditioning
the model on additional information (Mirza, M., &amp; Osindero, S. 2014).</p>
<p>This library also provides the Adversarial Auto-Encoders(AAEs),
which is a probabilistic Auto-Encoder that uses GANs to perform variational
inference by matching the aggregated posterior of the feature points
in hidden layer of the Auto-Encoder with an arbitrary prior
distribution(Makhzani, A., et al., 2015). Matching the aggregated posterior
to the prior ensures that generating from any part of prior space results
in meaningful samples. As a result, the decoder of the Adversarial Auto-Encoder
learns a deep generative model that maps the imposed prior to the data distribution.</p>
<p>On the other hand, models that construct a discriminator by Auto-Encoder have been proposed.
The Energy-based GAN(EBGAN) framework considers the discriminator as an energy function,
which assigns low energy values to real data and high to fake data. The generator is a
trainable parameterized function that produces samples in regions to which the discriminator
assigns low energy.</p>
<p>This class models the Energy-based Adversarial-Auto-Encoder(EBAAE) by structural coupling
between AAEs and EBGAN. The learning algorithm equivalents an adversarial training of AAEs as
a generator and EBGAN as a discriminator.</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Gauthier, J. (2014). Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, 2014(5), 2.</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … &amp; Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).</li>
<li>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., &amp; Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.</li>
<li>Mirza, M., &amp; Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.</li>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). Improved techniques for training gans. In Advances in neural information processing systems (pp. 2234-2242).</li>
<li>Zhao, J., Mathieu, M., &amp; LeCun, Y. (2016). Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126.</li>
<li>Warde-Farley, D., &amp; Bengio, Y. (2016). Improving generative adversarial networks with denoising feature matching.</li>
</ul>
<dl class="method">
<dt id="pygan.ebaae_image_generator.EBAAEImageGenerator.learn">
<code class="descname">learn</code><span class="sig-paren">(</span><em>iter_n=1000</em>, <em>k_step=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/ebaae_image_generator.html#EBAAEImageGenerator.learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.ebaae_image_generator.EBAAEImageGenerator.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learning.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>iter_n</strong> – <cite>int</cite> of the number of training iterations.</li>
<li><strong>k_step</strong> – <cite>int</cite> of the number of learning of the <cite>discriminative_model</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pygan.ebgan_image_generator">
<span id="pygan-ebgan-image-generator-module"></span><h2>pygan.ebgan_image_generator module<a class="headerlink" href="#module-pygan.ebgan_image_generator" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pygan.ebgan_image_generator.EBGANImageGenerator">
<em class="property">class </em><code class="descclassname">pygan.ebgan_image_generator.</code><code class="descname">EBGANImageGenerator</code><span class="sig-paren">(</span><em>dir_list</em>, <em>width=28</em>, <em>height=28</em>, <em>channel=1</em>, <em>initializer=None</em>, <em>batch_size=40</em>, <em>learning_rate=0.001</em>, <em>ctx=gpu(0)</em>, <em>discriminative_model=None</em>, <em>generative_model=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/ebgan_image_generator.html#EBGANImageGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.ebgan_image_generator.EBGANImageGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Image generation by EBGANs.</p>
<p>The Generative Adversarial Networks(GANs) (Goodfellow et al., 2014) framework establishes
a min-max adversarial game between two neural networks – a generative model, <cite>G</cite>, and a
discriminative model, <cite>D</cite>. The discriminator model, <cite>D(x)</cite>, is a neural network that computes
the probability that a observed data point x in data space is a sample from the data
distribution (positive samples) that we are trying to model, rather than a sample from our
generative model (negative samples).</p>
<p>Concurrently, the generator uses a function <cite>G(z)</cite> that maps samples <cite>z</cite> from the prior <cite>p(z)</cite> to
the data space. <cite>G(z)</cite> is trained to maximally confuse the discriminator into believing that
samples it generates come from the data distribution. The generator is trained by leveraging
the gradient of <cite>D(x)</cite> w.r.t. x, and using that to modify its parameters.</p>
<p>The Conditional GANs (or cGANs) is a simple extension of the basic GAN model which allows
the model to condition on external information. This makes it possible to engage the learned
generative model in different “modes” by providing it with different contextual
information (Gauthier, J. 2014).</p>
<p>This model can be constructed by simply feeding the data, <cite>y</cite>, to condition on to both the generator
and discriminator. In an unconditioned generative model, because the maps samples <cite>z</cite> from the prior
<cite>p(z)</cite> are drawn from uniform or normal distribution, there is no control on modes of the data being
generated. On the other hand, it is possible to direct the data generation process by conditioning
the model on additional information (Mirza, M., &amp; Osindero, S. 2014).</p>
<p>The Energy-based GAN framework considers the discriminator as an energy function,
which assigns low energy values to real data and high to fake data.
The generator is a trainable parameterized function that produces
samples in regions to which the discriminator assigns low energy.</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Gauthier, J. (2014). Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, 2014(5), 2.</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … &amp; Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).</li>
<li>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., &amp; Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.</li>
<li>Mirza, M., &amp; Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.</li>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). Improved techniques for training gans. In Advances in neural information processing systems (pp. 2234-2242).</li>
<li>Zhao, J., Mathieu, M., &amp; LeCun, Y. (2016). Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126.</li>
<li>Warde-Farley, D., &amp; Bengio, Y. (2016). Improving generative adversarial networks with denoising feature matching.</li>
</ul>
<dl class="method">
<dt id="pygan.ebgan_image_generator.EBGANImageGenerator.learn">
<code class="descname">learn</code><span class="sig-paren">(</span><em>iter_n=1000</em>, <em>k_step=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/ebgan_image_generator.html#EBGANImageGenerator.learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.ebgan_image_generator.EBGANImageGenerator.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learning.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>iter_n</strong> – <cite>int</cite> of the number of training iterations.</li>
<li><strong>k_step</strong> – <cite>int</cite> of the number of learning of the <cite>discriminative_model</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pygan.gan_image_generator">
<span id="pygan-gan-image-generator-module"></span><h2>pygan.gan_image_generator module<a class="headerlink" href="#module-pygan.gan_image_generator" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pygan.gan_image_generator.GANImageGenerator">
<em class="property">class </em><code class="descclassname">pygan.gan_image_generator.</code><code class="descname">GANImageGenerator</code><span class="sig-paren">(</span><em>dir_list</em>, <em>width=28</em>, <em>height=28</em>, <em>channel=1</em>, <em>initializer=None</em>, <em>batch_size=40</em>, <em>learning_rate=0.001</em>, <em>ctx=gpu(0)</em>, <em>discriminative_model=None</em>, <em>generative_model=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/gan_image_generator.html#GANImageGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.gan_image_generator.GANImageGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Image generation by GANs.</p>
<p>The Generative Adversarial Networks(GANs) (Goodfellow et al., 2014) framework establishes
a min-max adversarial game between two neural networks – a generative model, <cite>G</cite>, and a
discriminative model, <cite>D</cite>. The discriminator model, <cite>D(x)</cite>, is a neural network that computes
the probability that a observed data point <cite>x</cite> in data space is a sample from the data
distribution (positive samples) that we are trying to model, rather than a sample from our
generative model (negative samples).</p>
<p>Concurrently, the generator uses a function <cite>G(z)</cite> that maps samples <cite>z</cite> from the prior <cite>p(z)</cite> to
the data space. <cite>G(z)</cite> is trained to maximally confuse the discriminator into believing that
samples it generates come from the data distribution. The generator is trained by leveraging
the gradient of <cite>D(x)</cite> w.r.t. x, and using that to modify its parameters.</p>
<p>The Conditional GANs (or cGANs) is a simple extension of the basic GAN model which allows
the model to condition on external information. This makes it possible to engage the learned
generative model in different “modes” by providing it with different contextual
information (Gauthier, J. 2014).</p>
<p>This model can be constructed by simply feeding the data, <cite>y</cite>, to condition on to both the generator
and discriminator. In an unconditioned generative model, because the maps samples <cite>z</cite> from the prior
<cite>p(z)</cite> are drawn from uniform or normal distribution, there is no control on modes of the data being
generated. On the other hand, it is possible to direct the data generation process by conditioning
the model on additional information (Mirza, M., &amp; Osindero, S. 2014).</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Gauthier, J. (2014). Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, 2014(5), 2.</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … &amp; Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).</li>
<li>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., &amp; Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.</li>
<li>Mirza, M., &amp; Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.</li>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). Improved techniques for training gans. In Advances in neural information processing systems (pp. 2234-2242).</li>
<li>Zhao, J., Mathieu, M., &amp; LeCun, Y. (2016). Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126.</li>
<li>Warde-Farley, D., &amp; Bengio, Y. (2016). Improving generative adversarial networks with denoising feature matching.</li>
</ul>
<dl class="method">
<dt id="pygan.gan_image_generator.GANImageGenerator.learn">
<code class="descname">learn</code><span class="sig-paren">(</span><em>iter_n=1000</em>, <em>k_step=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pygan/gan_image_generator.html#GANImageGenerator.learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pygan.gan_image_generator.GANImageGenerator.learn" title="Permalink to this definition">¶</a></dt>
<dd><p>Learning.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>iter_n</strong> – <cite>int</cite> of the number of training iterations.</li>
<li><strong>k_step</strong> – <cite>int</cite> of the number of learning of the <cite>discriminative_model</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pygan">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pygan" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="modules.html" title="pygan"
             >next</a> |</li>
        <li class="right" >
          <a href="README.html" title="Generative Adversarial Networks Library: pygan"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pygan  documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Accel Brain.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>